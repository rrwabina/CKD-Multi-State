{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pyampute\n",
    "import pickle \n",
    "import time\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import tree\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "from app_hyperparameters import init_parameters_decision_tree, init_parameters_xgboost \n",
    "from app_hyperparameters import init_parameters_bayesian_ridge, init_parameters_random_forest\n",
    "from app_stopping_criteria import stop_iteration\n",
    "from app_uncertainty import uncertainty_sampling, multi_argmax, imputation_uncertainty\n",
    "from app_uncertainty import EI, remove_outliers\n",
    "from app_init import init_truncation, init_variable_schema, init_imputation_columns\n",
    "\n",
    "import random\n",
    "\n",
    "os.chdir('H:/Shared drives/CKD_Progression/')\n",
    "\n",
    "drive = 'H'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/save/qoc_cohort_ver002.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_multilevel(data, n_datasets, variable_schema, truncation, group_var, time_var, n_iteration = 50, \n",
    "                      convergence = ['maxit', 'delta', 'early_stop'], \n",
    "                      model       = ['linear', 'tree', 'forest', 'boost'], seed = 1997 + m, verbose = True):\n",
    "    \n",
    "    best_parameters = None\n",
    "    df_list = []\n",
    "    delta_change  = []\n",
    "    convergence_data = []\n",
    "    hyperparameters  = {}\n",
    "\n",
    "    for m in range(n_datasets):\n",
    "        if verbose:\n",
    "            print('Dataset number {:,}'.format(m + 1))\n",
    "        filled_df = data.copy()\n",
    "        mask = filled_df.isna()\n",
    "        \n",
    "        imp_order = filled_df[list(variable_schema.keys())].isna().sum().sort_values(ascending = False).index.tolist()\n",
    "\n",
    "        for to_impute in imp_order:\n",
    "            np.random.seed(seed + m)\n",
    "            local_mu = filled_df[to_impute].mean()\n",
    "            local_sd = filled_df[to_impute].std()\n",
    "            guesses = np.random.normal(loc   = local_mu, \n",
    "                                       scale = local_sd,\n",
    "                                       size  = len(filled_df))\n",
    "            filled_df.loc[mask[to_impute], to_impute] = guesses[mask[to_impute]]\n",
    "\n",
    "        grouped = filled_df.groupby(group_var)\n",
    "        it = 0\n",
    "        delta = []\n",
    "        post_iteration_distribution = []\n",
    "        \n",
    "        it_final, stop_condition = stop_iteration(filled_df, it, maxit = n_iteration, delta = delta, method = convergence, verbose = verbose)\n",
    "        if verbose:\n",
    "            print('Iteration', end = ' ')\n",
    "        while stop_condition: \n",
    "            if verbose:\n",
    "                print(str(it + 1), end = ', ')\n",
    "            \n",
    "            pre_it = filled_df[imp_order].copy()\n",
    "            for to_impute in imp_order:\n",
    "                for patient_id, patient_data in grouped:\n",
    "                    scaler = StandardScaler()\n",
    "                    sampled_data = patient_data[~mask.loc[patient_data.index, to_impute]]\n",
    "                    X_train = sampled_data[variable_schema[to_impute]]\n",
    "                    y_train = sampled_data[to_impute]\n",
    "\n",
    "                    for column in X_train.columns:\n",
    "                        if X_train[column].isnull().any():\n",
    "                            local_mu = filled_df[to_impute].mean()\n",
    "                            local_sd = filled_df[to_impute].std()\n",
    "                            \n",
    "                            guesses = np.random.normal(loc   = local_mu,\n",
    "                                                       scale = local_sd,\n",
    "                                                       size  = X_train[column].isnull().sum())\n",
    "                            X_train.loc[X_train[column].isnull(), column] = guesses\n",
    "\n",
    "                    if model == 'linear':\n",
    "                        estimator = BayesianRidge()   \n",
    "                        if it != 0:\n",
    "                            estimator.set_params(**best_parameters)\n",
    "\n",
    "                    elif model == 'tree':\n",
    "                        estimator = tree.DecisionTreeRegressor()\n",
    "                        if it != 0:\n",
    "                            estimator.set_params(**best_parameters)\n",
    "                            hyperparameters[model + f'{m}_dataset_{it}_iteration'] = best_parameters\n",
    "\n",
    "                    elif model == 'forest':\n",
    "                        estimator = RandomForestRegressor()\n",
    "                        if it != 0:\n",
    "                            estimator.set_params(**best_parameters)\n",
    "                            hyperparameters[model] = best_parameters\n",
    "\n",
    "                    elif model == 'boost':\n",
    "                        estimator = XGBRegressor()\n",
    "                        if it != 0:\n",
    "                            estimator.set_params(**best_parameters)\n",
    "                            hyperparameters[model] = best_parameters\n",
    "                      \n",
    "                    n_bootstraps = 200\n",
    "                    if m > 1:\n",
    "                        impute_columns = variable_schema[to_impute] + [to_impute]\n",
    "                        x_list = [df.loc[X_train.index, impute_columns] for df in df_list]\n",
    "                        uncertainty = imputation_uncertainty(x_list)\n",
    "                        mean        = np.mean(df_list[-1].loc[X_train.index, to_impute])\n",
    "                        maximum     = df_list[-1].loc[X_train.index, to_impute]\n",
    "                        improvement = EI(mean, uncertainty, maximum, tradeoff = 0.1)\n",
    "                        \n",
    "                        query_idx, _= multi_argmax(improvement, n_instances = X_train.shape[0])\n",
    "                        query_idx = [index for index in query_idx if index in X_train.index.tolist()]\n",
    "                        if not query_idx:\n",
    "                            query_idx = X_train.index.tolist()\n",
    "                        BOOTSTRAPPED  = np.empty((n_bootstraps, filled_df.shape[0]))\n",
    "                        for boot in range(n_bootstraps):\n",
    "                            np.random.seed(seed + m + m)  \n",
    "                            if len(query_idx) < 2:\n",
    "                                bootstrap_idx = X_train.index.tolist()\n",
    "                            else:    \n",
    "                                bootstrap_idx = np.random.choice(query_idx, size = len(query_idx), replace = True)\n",
    "\n",
    "                            estimator.fit(X_train.loc[bootstrap_idx, :], y_train.loc[bootstrap_idx])\n",
    "                            y_imputed = estimator.predict(filled_df[variable_schema[to_impute]])\n",
    "                            BOOTSTRAPPED[boot, :] = y_imputed\n",
    "                        y_imputed = BOOTSTRAPPED.mean(axis = 0)\n",
    "\n",
    "                    if m <= 1:\n",
    "                        estimator.fit(X_train, y_train)\n",
    "                        y_imputed = estimator.predict(patient_data[variable_schema[to_impute]])\n",
    "\n",
    "                    bounds = truncation[to_impute]\n",
    "                    y_imputed = np.full(len(patient_data), patient_data[to_impute].mean())\n",
    "                    y_imputed[y_imputed < bounds[0]] = bounds[0]\n",
    "                    y_imputed[y_imputed > bounds[1]] = bounds[1]\n",
    "                    y_imputed = winsorize(y_imputed, limits = (0.10, 0.10)) \n",
    "                    patient_idx = patient_data.index\n",
    "                    filled_df.loc[patient_idx, to_impute] = y_imputed\n",
    "\n",
    "                it = it + 1\n",
    "                post_it = filled_df[imp_order].copy()\n",
    "            \n",
    "                if convergence == 'delta':\n",
    "                   delta_val = ((post_it-pre_it)**2).sum() / ((post_it) ** 2).sum()\n",
    "                elif convergence == 'early_stop':\n",
    "                    delta_val = np.sqrt((((post_it - pre_it)**2)/len(post_it)).sum())\n",
    "                else:\n",
    "                    delta_val = pd.Series(np.NaN)\n",
    "            \n",
    "                delta.append(delta_val)\n",
    "                post_iteration_distribution.append(post_it)\n",
    "                it_final, stop_condition = stop_iteration(post_it, it, maxit = n_iteration, delta = delta, \n",
    "                                                          method = convergence, verbose = verbose)\n",
    "\n",
    "                penalty = int(np.array((pd.DataFrame(delta).median(axis = 0))).max())\n",
    "                if penalty <= 1:\n",
    "                    penalty = 2\n",
    "                loo = LeaveOneOut()\n",
    "                if stop_condition:\n",
    "                    if model == 'tree':\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        decision_tree_estimator = tree.DecisionTreeRegressor()\n",
    "                        randomized_search = RandomizedSearchCV(decision_tree_estimator, param_distributions = init_parameters_decision_tree(), \n",
    "                                                            cv = loo, n_iter = 1, n_jobs = 1)\n",
    "                        \n",
    "                        randomized_search.fit(X_train, y_train)\n",
    "                        best_parameters = randomized_search.best_params_\n",
    "\n",
    "                    elif model == 'boost':\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        xgboost_estimator = XGBRegressor(random_state = 42)\n",
    "                        randomized_search = RandomizedSearchCV(xgboost_estimator, param_distributions = init_parameters_xgboost(), \n",
    "                                                            cv = loo, n_iter = 2, n_jobs = -1)\n",
    "                        randomized_search.fit(X_train, y_train)\n",
    "                        best_parameters = randomized_search.best_params_\n",
    "\n",
    "                    elif model == 'linear':\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        linear_estimator  = BayesianRidge()\n",
    "                        randomized_search = RandomizedSearchCV(linear_estimator, param_distributions = init_parameters_bayesian_ridge(), \n",
    "                                                            cv = loo, n_iter = 2, n_jobs = -1)\n",
    "                        randomized_search.fit(X_train, y_train)\n",
    "                        best_parameters = randomized_search.best_params_      \n",
    "\n",
    "                    elif model == 'forest':\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        random_forest_estimator  = RandomForestRegressor()\n",
    "                        randomized_search = RandomizedSearchCV(random_forest_estimator, \n",
    "                                                            param_distributions = init_parameters_random_forest(), \n",
    "                                                            cv = loo, n_iter = 2, n_jobs = 10)\n",
    "                        randomized_search.fit(X_train, y_train)\n",
    "                        best_parameters = randomized_search.best_params_                 \n",
    "                \n",
    "            df_list.append(filled_df)\n",
    "            delta_change.append(pd.concat(delta, axis = 'columns'))\n",
    "            convergence_data.append(post_iteration_distribution)\n",
    "    result = {'imputed_data':       df_list, \n",
    "              'convergence_data':   convergence_data, \n",
    "              'iteration_delta':    delta_change}\n",
    "    return result, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute = init_imputation_columns()\n",
    "main_data = pd.read_csv(docs_path + 'CKD_TVC_IMPUTATION_14December2024.csv')\n",
    "main_data = main_data.rename(columns = {'height': 'HIGH'})\n",
    "main_data = main_data[['ENC_HN', 'modulo_365'] + columns_impute]\n",
    "main_data = remove_outliers(main_data, 'BMI')\n",
    "main_data = remove_outliers(main_data, 'HIGH')\n",
    "main_data = main_data[main_data['modulo_365'] != 14]\n",
    "\n",
    "mask = main_data['modulo_365'] == 13\n",
    "main_data.loc[mask, 'Renal_Serum_creatinine'] = guess = np.random.normal(loc   = main_data['Renal_Serum_creatinine'].mean(), \n",
    "                                                                         scale = main_data['Renal_Serum_creatinine'].std(), \n",
    "                                                                         size  = len(main_data[mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear early_stop\n",
      "linear model with early_stop convergence, Dataset number 1\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 2\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 3\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 4\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 5\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 6\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 7\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 8\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 9\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 10\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 11\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 12\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 13\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 14\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 15\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 16\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 17\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 18\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 19\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 20\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 21\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 22\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 23\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 24\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 25\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 26\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 27\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 28\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 29\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 30\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 31\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 32\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 33\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 34\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 35\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 36\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 37\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 38\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 39\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 40\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 41\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 42\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 43\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 44\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 45\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 46\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 47\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 48\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 49\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 50\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 51\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 52\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 53\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 54\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 55\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 56\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 57\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 58\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 59\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 60\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 61\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 62\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 63\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 64\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 65\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 66\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 67\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 68\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 69\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 70\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 71\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 72\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 73\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 74\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 75\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 76\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 77\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 78\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 79\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "Dataset number 80\n",
      "Iteration 1, stopping at 10 iteration(s)\n",
      "time taken to impute 39402.2236 seconds\n"
     ]
    }
   ],
   "source": [
    "def impute_loop(data, model, convergence, path):\n",
    "    name = model + \"_\" + convergence\n",
    "    path = path+\"/\"+name+\"_80D_10I.pickle\"\n",
    "    imputed_data, hyperparameters = impute_multilevel(data = data, \n",
    "                                                      n_datasets = 80, \n",
    "                                                      variable_schema = init_variable_schema(), \n",
    "                                                      truncation = init_truncation(), \n",
    "                                                      group_var = 'modulo_365',\n",
    "                                                      time_var  = 'ENC_HN',\n",
    "                                                      n_iteration = 10, \n",
    "                                                      convergence = convergence, \n",
    "                                                      seed = 1996 + 10, \n",
    "                                                      model = model, \n",
    "                                                      verbose = True)\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(imputed_data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    return hyperparameters\n",
    "\n",
    "models = ['linear'] \n",
    "  \n",
    "records = []\n",
    "for model in models:\n",
    "    for convergence in ['early_stop']:\n",
    "        print(model, convergence)\n",
    "        print('{} model with {} convergence'.format(model, convergence), end = ', ')\n",
    "        start = time.time()\n",
    "        hyperparameters = impute_loop(data = main_data, model = model, convergence = convergence, \n",
    "                                      path = 'H:/Shared drives/CKD_Progression/result/tvc_imputation/')\n",
    "        stop = time.time()\n",
    "        print('time taken to impute {:.4f} seconds'.format(stop - start))\n",
    "        records.append([model, convergence, 'sequential',  stop - start])\n",
    "\n",
    "temp = pd.Series(records)\n",
    "temp.columns = ['model', 'convergence', 'datasets', 'time_taken']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
