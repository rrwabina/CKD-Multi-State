{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pyampute\n",
    "import pickle \n",
    "import time\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import tree\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from app_hyperparameters import init_parameters_decision_tree, init_parameters_xgboost \n",
    "from app_hyperparameters import init_parameters_bayesian_ridge, init_parameters_random_forest\n",
    "from app_stopping_criteria import stop_iteration\n",
    "from app_uncertainty import uncertainty_sampling, multi_argmax, imputation_uncertainty\n",
    "from app_uncertainty import EI\n",
    "from app_init import init_truncation, init_variable_schema\n",
    "\n",
    "import miceforest as mf \n",
    "import random\n",
    "\n",
    "os.chdir('H:/Shared drives/CKD_Progression/')\n",
    "\n",
    "drive = 'H'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/save/qoc_cohort_ver002.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute(data, n_datasets, variable_schema, truncation, n_iteration = 50, \n",
    "           convergence = ['maxit', 'delta', 'early_stop'], \n",
    "           model       = ['linear', 'tree', 'forest', 'boost'], seed = 1997, verbose = True):\n",
    "    \n",
    "    best_parameters = None\n",
    "    df_list = []\n",
    "    delta_change  = []\n",
    "    convergence_data = []\n",
    "    hyperparameters  = {}\n",
    "    for m in range(n_datasets):\n",
    "        if verbose:\n",
    "            print('Dataset number {:,}'.format(m + 1))\n",
    "        filled_df = data.copy()\n",
    "        mask = filled_df.isna()\n",
    "        \n",
    "        imp_order = filled_df[list(variable_schema.keys())].isna().sum().sort_values(ascending=False).index\n",
    "        for to_impute in imp_order:\n",
    "            np.random.seed(seed * m)\n",
    "            guess = np.random.normal(loc = filled_df[to_impute].mean(), scale = filled_df[to_impute].std(), size = len(filled_df))\n",
    "            filled_df[to_impute] = filled_df[to_impute].fillna(pd.Series(guess))\n",
    "        it = 0\n",
    "        delta = []\n",
    "        post_iteration_distribution = []\n",
    "        it_final, stop_condition = stop_iteration(filled_df, it, maxit = n_iteration, delta = delta, method = convergence, verbose = verbose)\n",
    "        if verbose:\n",
    "            print('Iteration', end = ' ')\n",
    "        while stop_condition: \n",
    "            if verbose:\n",
    "                print(str(it + 1), end = ', ')\n",
    "            pre_it = filled_df[imp_order].copy()\n",
    "            for to_impute in imp_order:\n",
    "                scaler = StandardScaler()\n",
    "                sampled_data = pre_it[~mask[to_impute]]\n",
    "                X_train = sampled_data[variable_schema[to_impute]]\n",
    "                y_train = sampled_data[to_impute]\n",
    " \n",
    "                if model == 'linear':\n",
    "                    estimator = BayesianRidge()   \n",
    "                    if it != 0:\n",
    "                        estimator.set_params(**best_parameters)\n",
    "\n",
    "                elif model == 'tree':\n",
    "                    estimator = tree.DecisionTreeRegressor()\n",
    "                    if it != 0:\n",
    "                        estimator.set_params(**best_parameters)\n",
    "                        hyperparameters[model + f'{m}_dataset_{it}_iteration'] = best_parameters\n",
    "\n",
    "                elif model == 'forest':\n",
    "                    estimator = RandomForestRegressor()\n",
    "                    if it != 0:\n",
    "                        estimator.set_params(**best_parameters)\n",
    "                        hyperparameters[model] = best_parameters\n",
    "\n",
    "                elif model == 'boost':\n",
    "                    estimator = XGBRegressor()\n",
    "                    if it != 0:\n",
    "                        estimator.set_params(**best_parameters)\n",
    "                        hyperparameters[model] = best_parameters\n",
    "                      \n",
    "                # Expected Improvement (with bootstrapping)\n",
    "                n_bootstraps = 200\n",
    "                if m > 1:\n",
    "                    impute_columns = variable_schema[to_impute] + [to_impute]\n",
    "                    x_list = [df.loc[X_train.index, impute_columns] for df in df_list]\n",
    "                    uncertainty = imputation_uncertainty(x_list)\n",
    "                    mean        = np.mean(df_list[-1].loc[X_train.index, to_impute])\n",
    "                    maximum     = df_list[-1].loc[X_train.index, to_impute]\n",
    "                    improvement = EI(mean, uncertainty, maximum, tradeoff = 0.1)\n",
    "                    query_idx, _= multi_argmax(improvement, n_instances = X_train.shape[0])\n",
    "                    query_idx = [index for index in query_idx if index in X_train.index.tolist()]\n",
    "                    BOOTSTRAPPED  = np.empty((n_bootstraps, filled_df.shape[0]))\n",
    "                    for boot in range(n_bootstraps):\n",
    "                        bootstrap_idx = np.random.choice(query_idx, size = len(query_idx), replace = True)\n",
    "                        estimator.fit(X_train.loc[bootstrap_idx, :], y_train.loc[bootstrap_idx])\n",
    "                        y_imputed = estimator.predict(filled_df[variable_schema[to_impute]])\n",
    "                        BOOTSTRAPPED[boot, :] = y_imputed\n",
    "                    y_imputed = BOOTSTRAPPED.mean(axis = 0)\n",
    "\n",
    "                if m <= 1:\n",
    "                    estimator.fit(X_train, y_train)\n",
    "                    y_imputed = estimator.predict(filled_df[variable_schema[to_impute]])\n",
    "                bounds = truncation[to_impute]\n",
    "                y_imputed[y_imputed < bounds[0]] = bounds[0]\n",
    "                y_imputed[y_imputed > bounds[1]] = bounds[1]\n",
    "                y_imputed = winsorize(y_imputed, limits = (0.10, 0.10)) \n",
    "                filled_df.loc[mask[to_impute], to_impute] = y_imputed[mask[to_impute]]\n",
    "                \n",
    "            it = it + 1\n",
    "            post_it = filled_df[imp_order].copy()\n",
    "            if convergence == 'delta':\n",
    "                delta_val = ((post_it-pre_it)**2).sum() / ((post_it) ** 2).sum()\n",
    "            elif convergence == 'early_stop':\n",
    "                delta_val = np.sqrt((((post_it - pre_it)**2)/len(post_it)).sum())\n",
    "            else:\n",
    "                delta_val = pd.Series(np.NaN)\n",
    "            delta.append(delta_val)\n",
    "            post_iteration_distribution.append(post_it)\n",
    "            it_final, stop_condition = stop_iteration(post_it, it, maxit = n_iteration, delta = delta, \n",
    "                                                      method = convergence, verbose = verbose)\n",
    "\n",
    "            penalty = int(np.array((pd.DataFrame(delta).median(axis = 0))).max())\n",
    "            if penalty <= 1:\n",
    "                penalty = 2\n",
    "        \n",
    "            if stop_condition:\n",
    "                if model == 'tree':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    decision_tree_estimator = tree.DecisionTreeRegressor()\n",
    "                    randomized_search = RandomizedSearchCV(decision_tree_estimator, param_distributions = init_parameters_decision_tree(), \n",
    "                                                           cv = 10, n_iter = 1, n_jobs = 1)\n",
    "                    \n",
    "                    randomized_search.fit(X_train, y_train)\n",
    "                    best_parameters = randomized_search.best_params_\n",
    "\n",
    "                elif model == 'boost':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    xgboost_estimator = XGBRegressor(random_state = 42)\n",
    "                    randomized_search = RandomizedSearchCV(xgboost_estimator, param_distributions = init_parameters_xgboost(), \n",
    "                                                           cv = 10, n_iter = 2, n_jobs = -1)\n",
    "                    randomized_search.fit(X_train, y_train)\n",
    "                    best_parameters = randomized_search.best_params_\n",
    "\n",
    "                elif model == 'linear':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    linear_estimator  = BayesianRidge()\n",
    "                    randomized_search = RandomizedSearchCV(linear_estimator, param_distributions = init_parameters_bayesian_ridge(), \n",
    "                                                           cv = 10, n_iter = 2, n_jobs = -1)\n",
    "                    randomized_search.fit(X_train, y_train)\n",
    "                    best_parameters = randomized_search.best_params_      \n",
    "\n",
    "                elif model == 'forest':\n",
    "                    scaler = StandardScaler()\n",
    "                    X_train = scaler.fit_transform(X_train)\n",
    "                    random_forest_estimator  = RandomForestRegressor()\n",
    "                    randomized_search = RandomizedSearchCV(random_forest_estimator, \n",
    "                                                           param_distributions = init_parameters_random_forest(), \n",
    "                                                           cv = 10, n_iter = 2, n_jobs = 10)\n",
    "                    randomized_search.fit(X_train, y_train)\n",
    "                    best_parameters = randomized_search.best_params_                 \n",
    "        df_list.append(filled_df)\n",
    "        delta_change.append(pd.concat(delta, axis = 'columns'))\n",
    "        convergence_data.append(post_iteration_distribution)\n",
    "    result = {'imputed_data': df_list, 'convergence_data': convergence_data, 'iteration_delta': delta_change}\n",
    "    return result, hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute = ['BMI', 'BW', 'HIGH', 'Lipid_HDL', 'Lipid_LDL', 'Chem_glucose', 'Chem_HbA1C', 'Renal_Uric_acid', \n",
    "                  'Lipid_Cholesterol', 'Renal_Serum_creatinine', 'Lipid_Triglyceride', 'age', 'Renal_eGFR', 'CVD']\n",
    "\n",
    "main_data = pd.read_csv(save_path + 'qoc_cohort_ver002.csv')\n",
    "main_data = main_data.rename(columns = {'height': 'HIGH'})\n",
    "main_data = main_data[columns_impute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_loop(data, model, convergence, path):\n",
    "    name = model + \"_\" + convergence\n",
    "    path = path+\"/\"+name+\"_40D_10I_ver2.pickle\"\n",
    "    imputed_data, hyperparameters = impute(data = data, n_datasets = 40, variable_schema = init_variable_schema(), \n",
    "                          truncation = init_truncation(), n_iteration = 10, convergence = convergence, \n",
    "                          seed = 1996, model = model, verbose = True)\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(imputed_data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    return hyperparameters\n",
    "\n",
    "models = ['tree']\n",
    "records = []\n",
    "for model in models:\n",
    "    for convergence in ['early_stop']:\n",
    "        print(model, convergence)\n",
    "        print('{} model with {} convergence'.format(model, convergence), end = ', ')\n",
    "        start = time.time()\n",
    "        hyperparameters = impute_loop(data = main_data, model = model, convergence = convergence, \n",
    "                                      path = 'H:/Shared drives/CKD_Progression/result/imputation/')\n",
    "        stop = time.time()\n",
    "        print('time taken to impute {:.4f} seconds'.format(stop - start))\n",
    "        records.append([model, convergence, 'sequential',  stop - start])\n",
    "\n",
    "temp = pd.Series(records)\n",
    "temp.columns = ['model', 'convergence', 'datasets', 'time_taken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree early_stop\n",
      "tree model with early_stop convergence, Dataset number 1\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 2\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 3\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 4\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 5\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 6\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 7\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 8\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 9\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 10\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 11\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 12\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 13\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 14\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 15\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 16\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 17\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 18\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 19\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 20\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 21\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 22\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 23\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 24\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 25\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 26\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 27\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 28\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 29\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 30\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 31\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 32\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 33\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 34\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 35\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 36\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 37\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 38\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 39\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "Dataset number 40\n",
      "Iteration 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, stopping at 10 iteration(s)\n",
      "time taken to impute 596254.5102 seconds\n"
     ]
    }
   ],
   "source": [
    "def impute_loop(data, model, convergence, path):\n",
    "    name = model + \"_\" + convergence\n",
    "    path = path+\"/\"+name+\"_40D_10I.pickle\"\n",
    "    imputed_data, hyperparameters = impute(data = data, n_datasets = 40, variable_schema = init_variable_schema(), \n",
    "                          truncation = init_truncation(), n_iteration = 10, convergence = convergence, \n",
    "                          seed = 1996, model = model, verbose = True)\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(imputed_data, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    return hyperparameters\n",
    "\n",
    "models = ['tree']\n",
    "records = []\n",
    "for model in models:\n",
    "    for convergence in ['early_stop']:\n",
    "        print(model, convergence)\n",
    "        print('{} model with {} convergence'.format(model, convergence), end = ', ')\n",
    "        start = time.time()\n",
    "        hyperparameters = impute_loop(data = main_data, model = model, convergence = convergence, \n",
    "                                      path = 'H:/Shared drives/CKD_Progression/result/imputation/')\n",
    "        stop = time.time()\n",
    "        print('time taken to impute {:.4f} seconds'.format(stop - start))\n",
    "        records.append([model, convergence, 'sequential',  stop - start])\n",
    "\n",
    "temp = pd.Series(records)\n",
    "temp.columns = ['model', 'convergence', 'datasets', 'time_taken']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
