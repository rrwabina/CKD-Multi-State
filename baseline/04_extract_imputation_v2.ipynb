{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pyampute\n",
    "import pickle \n",
    "import time\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import tree\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from app_hyperparameters import init_parameters_decision_tree, init_parameters_xgboost \n",
    "from app_hyperparameters import init_parameters_bayesian_ridge, init_parameters_random_forest\n",
    "from app_stopping_criteria import stop_iteration\n",
    "from app_uncertainty import uncertainty_sampling, multi_argmax, imputation_uncertainty\n",
    "from app_uncertainty import EI\n",
    "from app_init import init_truncation, init_variable_schema\n",
    "\n",
    "import miceforest as mf \n",
    "import random\n",
    "\n",
    "os.chdir('M:/Shared drives/CKD_Progression/')\n",
    "\n",
    "drive = 'M'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/save/qoc_cohort_ver002.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute = ['BMI', 'BW', 'HIGH', 'Lipid_HDL', 'Lipid_LDL', 'Chem_glucose', 'Chem_HbA1C', 'Renal_Uric_acid', \n",
    "                  'Lipid_Cholesterol', 'Renal_Serum_creatinine', 'Lipid_Triglyceride', 'age', 'Renal_eGFR', 'CVD']\n",
    "\n",
    "main_data = pd.read_csv(save_path + 'qoc_cohort_ver002.csv')\n",
    "main_data = main_data.rename(columns = {'height': 'HIGH'})\n",
    "main_data = main_data[columns_impute]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration_metrics(data, plot_type, log=False, show=True, export_path=False):\n",
    "    for i in range(len(data)):\n",
    "        g = sns.lineplot(data.iloc[i,:])\n",
    "        g.set_xticks(range(len(data.columns))) \n",
    "        g.set_xticklabels([str(int(i)+1) for i in data.columns])\n",
    "        \n",
    "        g.set_xlabel(\"number of iterations\")\n",
    "        g.set_ylabel(plot_type)\n",
    "        if log:\n",
    "            plt.yscale(\"log\")\n",
    "    if export_path:\n",
    "        plt.savefig(export_path+\"/\"+\"_\".join(plot_type.split(\" \"))+\".png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.clf()\n",
    "\n",
    "def conv_metric(col, func, convergence_data):\n",
    "    def calc(data, func=[\"mean\", \"std\"]):\n",
    "        if func == \"mean\":\n",
    "            return data.mean()\n",
    "        elif func == \"std\":\n",
    "            return data.std()\n",
    "    temp = pd.DataFrame([[calc(i[col], \"mean\") for i in imp_data] for imp_data in convergence_data])\n",
    "    return temp\n",
    "    \n",
    "def convergence_plot(features, convergence_data, func = [\"mean\", \"std\"], show=False, export_path=False):\n",
    "    for col in features:\n",
    "        temp = conv_metric(col=col, func=func, convergence_data=convergence_data)\n",
    "        iteration_metrics(temp, plot_type=col+\" \"+func+\" convergence\", show=show, export_path=export_path)\n",
    "\n",
    "def delta_metric(col, delta_change):\n",
    "    temp = pd.concat([data[data.index == col] for data in delta_change]).reset_index(drop=True)\n",
    "    return temp\n",
    "    \n",
    "def delta_plot(features, delta_change, log=False, show=True, export_path=False):\n",
    "    for col in features:\n",
    "        temp = delta_metric(col=col, delta_change=delta_change)\n",
    "        iteration_metrics(temp, plot_type = col+\" delta\", log = log, show=show, export_path=export_path)\n",
    "\n",
    "def density_plot(imputed, features, amputed=False, reference=False, show=False, export_path=False):\n",
    "    for col in features:\n",
    "        if reference is not False:\n",
    "            sns.kdeplot(reference[col], color=\"g\", fill=True, linewidth=0, alpha=0.8, label=\"Reference data\")\n",
    "        if amputed is not False:\n",
    "            sns.kdeplot(amputed[col], color=\"b\", label = \"Amputated data\")\n",
    "        for i in range(len(imputed)):\n",
    "            if i==0:\n",
    "                sns.kdeplot(imputed[i][col], color=\"r\", alpha=0.5, label=\"Imputed data\")\n",
    "            else:\n",
    "                sns.kdeplot(imputed[i][col], color=\"r\", alpha=0.5)\n",
    "        plt.legend()\n",
    "        if export_path:            \n",
    "            plt.savefig(export_path+\"/\"+col+\".png\")\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imputed(model, convergence, path):\n",
    "    name = model + \"_\" + convergence\n",
    "    path = path+\"/\"+name+\".pickle\"\n",
    "    with open(path, \"rb\") as handle:\n",
    "        imputed_data = pickle.load(handle)\n",
    "    return imputed_data\n",
    "\n",
    "def calculate_summary_statistics(df):\n",
    "    mean_values   = df.mean()\n",
    "    std_values    = df.std()\n",
    "    median_values = df.median()\n",
    "    iqr_values    = df.quantile(0.75) - df.quantile(0.25)\n",
    "    summary_stats = pd.DataFrame({\n",
    "        'Mean': mean_values,\n",
    "        'Standard Deviation': std_values,\n",
    "        'Median': median_values,\n",
    "        'IQR': iqr_values})\n",
    "    return summary_stats\n",
    "\n",
    "def export_imputed(model, convergence, features, path, verbose=True):\n",
    "    print('Exporting for {} model with {} convergence'.format(model, convergence))\n",
    "    name = model + '_' + convergence\n",
    "    dst_dir = path+'/'+name+'/'\n",
    "    if verbose:\n",
    "        print('Exporting datasets', end = ', ')\n",
    "    os.makedirs(os.path.dirname(dst_dir + 'datasets/'), exist_ok = True)\n",
    "    imputed_data = load_imputed(model = model, convergence = convergence, path = path)\n",
    "    \n",
    "    df_list = imputed_data[0]['imputed_data']\n",
    "    convergence_data = imputed_data[0]['convergence_data']\n",
    "    delta_change = imputed_data[0]['iteration_delta']\n",
    "\n",
    "    df_list = imputed_data[0]['imputed_data']\n",
    "    stacked_df = np.stack([df.to_numpy() for df in df_list], axis=2)\n",
    "    averaged_array = np.nanmean(stacked_df, axis=2)\n",
    "    averaged_df = pd.DataFrame(averaged_array, columns=df_list[0].columns)\n",
    "\n",
    "    missing_bmi_mask = averaged_df['BMI'].isna()\n",
    "    calculated_bmi = averaged_df.loc[missing_bmi_mask, 'BW'] / (averaged_df.loc[missing_bmi_mask, 'HIGH'] / 100) ** 2\n",
    "    averaged_df.loc[missing_bmi_mask, 'BMI'] = calculated_bmi\n",
    "\n",
    "    summary_statistics = calculate_summary_statistics(averaged_df)\n",
    "    maindat_statistics = calculate_summary_statistics(main_data)\n",
    "\n",
    "    mean_summary = []\n",
    "    median_summary = []\n",
    "    for i in range(len(df_list)):\n",
    "        complete_baseline = df_list[i]\n",
    "\n",
    "        imp_summary = complete_baseline.describe()[columns_impute].T[['mean', 'std']].apply(lambda row: [round(i, 2) for i in row], axis = 1)\n",
    "        mean_summary  .append(imp_summary)\n",
    "        imp_summary = complete_baseline.describe()[columns_impute].T[['50%', '25%', '75%']].apply(lambda row: [round(i, 2) for i in row], axis = 1)\n",
    "        median_summary.append(imp_summary)\n",
    "\n",
    "    rmse  = np.sqrt((pd.concat([((data[columns_impute] - main_data[columns_impute]).mean())**2 for data in df_list], axis = 'columns')).sum(1)/len(df_list))\n",
    "    mae   = pd.concat([abs(data[columns_impute] - main_data[columns_impute]).mean() for data in df_list], axis = 'columns')\n",
    "\n",
    "    mean_summary = pd.concat(mean_summary, axis = 'columns')\n",
    "    mean_summary['mean'] = mean_summary.iloc[:,:len(df_list)].apply(lambda row: np.mean([i[0] for i in row]), axis = 1)\n",
    "    mean_summary['sd']   = mean_summary.iloc[:,:len(df_list)].apply(lambda row: np.mean([i[1] for i in row]), axis = 1)\n",
    "    mean_summary['rmse'] = rmse\n",
    "\n",
    "    median_summary = pd.concat(median_summary, axis = 'columns')\n",
    "    median_summary['median'] = median_summary.iloc[:,:len(df_list)].apply(lambda row: np.mean([i[0] for i in row]), axis = 1)\n",
    "    median_summary['25']     = median_summary.iloc[:,:len(df_list)].apply(lambda row: np.mean([i[1] for i in row]), axis = 1)\n",
    "    median_summary['75']     = median_summary.iloc[:,:len(df_list)].apply(lambda row: np.mean([i[2] for i in row]), axis = 1)\n",
    "    median_summary['rmse']   = rmse\n",
    "    \n",
    "    mean_summary  .to_csv(dst_dir + 'mean_distribution.csv')\n",
    "    median_summary.to_csv(dst_dir + 'median_distribution.csv')\n",
    "    \n",
    "    summary_statistics.to_csv(dst_dir + 'summary_statistics.csv', index = False)\n",
    "    maindat_statistics.to_csv(dst_dir + 'primary_statistics.csv', index = False)\n",
    "    averaged_df.to_csv(dst_dir + 'IMPUTED_LinearReg.csv', index = False)\n",
    "\n",
    "    if verbose:\n",
    "        print('Convergence plots', end = ', ')\n",
    "    dst_dir = path + '/' + name + '/'\n",
    "    os.makedirs(os.path.dirname(dst_dir + 'convergence_plot/'), exist_ok = True)\n",
    "    convergence_plot(columns_impute, convergence_data = df_list, func = 'mean', show = False, export_path = dst_dir + 'convergence_plot/')\n",
    "\n",
    "    if verbose:\n",
    "        print('Density plots', end = ', ')\n",
    "    dst_dir = path + '/' + name + '/'\n",
    "    os.makedirs(os.path.dirname(dst_dir + \"density_plot/\"), exist_ok = True)  \n",
    "    density_plot(df_list, columns_impute, amputed = main_data, reference = main_data, show = False, export_path = dst_dir + \"density_plot/\")\n",
    "\n",
    "    if convergence != 'maxit':\n",
    "        if verbose:\n",
    "            print('delta plots')\n",
    "        dst_dir = path + '/' + name + '/'\n",
    "        os.makedirs(os.path.dirname(dst_dir + 'delta_plot/'), exist_ok = True)  \n",
    "        delta_plot(columns_impute, delta_change, log = False, show = False, export_path = dst_dir + 'delta_plot/')\n",
    "    else:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting for linear model with early_stop convergence\n",
      "Exporting datasets, "
     ]
    }
   ],
   "source": [
    "path = 'M:/Shared drives/CKD_Progression/result/imputation/'\n",
    "export_imputed('linear', 'early_stop', columns_impute, path, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
