{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pykalman import KalmanFilter\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pyampute\n",
    "import pickle \n",
    "import time\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import tree\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from data import load_dataset \n",
    "from app_hyperparameters import init_parameters_decision_tree, init_parameters_xgboost \n",
    "from app_hyperparameters import init_parameters_bayesian_ridge, init_parameters_random_forest\n",
    "from app_stopping_criteria import stop_iteration\n",
    "from app_uncertainty import uncertainty_sampling, multi_argmax, imputation_uncertainty\n",
    "from app_uncertainty import EI\n",
    "from app_init import init_truncation, init_variable_schema\n",
    "from app_init import init_truncation, init_variable_schema, init_imputation_columns\n",
    "\n",
    "import miceforest as mf \n",
    "import random\n",
    "\n",
    "drive = 'H'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/data/CKD_COHORT_Jan2010_Mar2024_v3.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "resu_path = drive + ':/Shared drives/CKD_Progression/result/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int_list(input_str):\n",
    "    result_list = []\n",
    "    for i, char in enumerate(input_str):\n",
    "        if char == '0':\n",
    "            continue\n",
    "        else:\n",
    "            result_list.append(f'{i} ')\n",
    "    return result_list\n",
    "\n",
    "def generate_miss_pattern(df, columns, mechanism = 'MAR', threshold=False, top = False):\n",
    "    '''\n",
    "    Parameters:\n",
    "        df (dataframe): original dataframe  (with missing values)\n",
    "        columns (list): columns with missing values\n",
    "        mechanism (string): MAR (default)\n",
    "                            MNAR: missing not at random\n",
    "        top (int): top n patterns \n",
    "    Returns result (list): a list of dictionaries with its corresponding missingness pattern and frequency\n",
    "    '''\n",
    "    miss_pattern = df[columns]\n",
    "    miss_pattern = miss_pattern.isna().astype(int)\n",
    "    miss_pattern.columns = np.arange(1, len(miss_pattern.columns) + 1, 1).tolist()\n",
    "\n",
    "    miss_pattern['combined'] = miss_pattern.apply(lambda row: ''.join(map(str, row)), axis = 1)\n",
    "    miss_pattern['combined'] = miss_pattern['combined'].apply(convert_to_int_list)\n",
    "    \"if there are complete rows, don't count them\"\n",
    "    miss_pattern = miss_pattern[miss_pattern[\"combined\"].apply(lambda x: len(x)>0)]\n",
    "    \n",
    "    miss_pattern = pd.DataFrame(miss_pattern['combined'].value_counts(ascending = False))\n",
    "    if top is not False:\n",
    "        miss_pattern = miss_pattern.head(top)\n",
    "    miss_pattern['freq'] = miss_pattern['combined']/miss_pattern['combined'].sum()\n",
    "    \"if we use threshold, remove rows lower than that, then recalculate frequencies\"\n",
    "    if threshold is not False:\n",
    "        miss_pattern = miss_pattern[miss_pattern['freq']>=threshold]\n",
    "        miss_pattern['freq'] = miss_pattern['combined']/miss_pattern['combined'].sum()\n",
    "    miss_pattern = miss_pattern.reset_index().rename(columns = {'index': 'incomplete_vars'})\n",
    "    miss_pattern = miss_pattern.drop('combined', axis = 1)\n",
    "    miss_pattern['mechanism'] = mechanism\n",
    "\n",
    "    result = []\n",
    "    for index, row in miss_pattern.iterrows():\n",
    "        incomplete_vars = row['incomplete_vars']\n",
    "        result.append({\n",
    "            'incomplete_vars': list(set(map(int, row['incomplete_vars']))),\n",
    "            'mechanism': row['mechanism'],\n",
    "            'freq': row['freq']})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_miss_pattern(df, columns, mechanism='MAR', threshold=False, top=False):\n",
    "    miss_pattern = df[columns].isna().to_numpy().astype(int)\n",
    "    combined = np.dot(miss_pattern, 1 << np.arange(miss_pattern.shape[1] - 1, -1, -1))\n",
    "\n",
    "    unique, counts = np.unique(combined, return_counts=True)\n",
    "    freq = counts / counts.sum()\n",
    "\n",
    "    if threshold:\n",
    "        mask = freq >= threshold\n",
    "        unique, counts, freq = unique[mask], counts[mask], freq[mask]\n",
    "\n",
    "    if top:\n",
    "        top_indices = np.argsort(counts)[-top:][::-1]\n",
    "        unique, freq = unique[top_indices], freq[top_indices]\n",
    "\n",
    "    result = [{'incomplete_vars': [i + 1 for i in range(len(columns)) if (u >> i) & 1],\n",
    "               'mechanism': mechanism,\n",
    "               'freq': f} for u, f in zip(unique, freq)]\n",
    "    return result\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates, order_covariates, long_df = load_dataset(get_columns = True)\n",
    "patients = long_df['ENC_HN'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute = init_imputation_columns()\n",
    "df = pd.read_csv(docs_path + 'CKD_TVC_IMPUTATION_14December2024.csv')\n",
    "df = df.rename(columns = {'height': 'HIGH'})\n",
    "df = df[['ENC_HN', 'modulo_365'] + columns_impute]\n",
    "df = remove_outliers(df, 'BMI')\n",
    "df = df[df['modulo_365'] != 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(percentiles = [0.01, 0.10, 0.25, 0.50, 0.75, 0.99]).T.to_csv(docs_path + 'descriptive_longitudinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = df.groupby('ENC_HN').apply(\n",
    "    lambda group: group.isna().mean())\n",
    "missing_rate = missing_rate.drop(columns = ['modulo_365', 'ENC_HN']).reset_index(drop = True)\n",
    "missing_rate.describe().T.to_csv(docs_path + 'missing_rate_longitudinal_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_miss_pattern(df, columns, mechanism = 'MAR', threshold = False, top = False):\n",
    "    miss_pattern = df[columns].isna().to_numpy(dtype=np.uint8)\n",
    "    powers = 1 << np.arange(miss_pattern.shape[1] - 1, -1, -1, dtype=np.int64)\n",
    "    combined = miss_pattern.dot(powers).astype(np.int64)\n",
    "\n",
    "    unique, counts = np.unique(combined, return_counts=True)\n",
    "    total_count = counts.sum()\n",
    "    freq = counts / total_count\n",
    "\n",
    "    if threshold:\n",
    "        mask = freq >= threshold\n",
    "        unique, freq = unique[mask], freq[mask]\n",
    "\n",
    "    if top:\n",
    "        top_indices  = np.argpartition(counts, -top)[-top:]\n",
    "        unique, freq = unique[top_indices], freq[top_indices]\n",
    "        sorted_order = np.argsort(-counts[top_indices])\n",
    "        unique, freq = unique[sorted_order], freq[sorted_order]\n",
    "\n",
    "    result = [\n",
    "        {\n",
    "            'incomplete_vars': [i + 1 for i in range(len(columns)) if (u >> i) & 1],\n",
    "            'mechanism': mechanism,\n",
    "            'freq': f\n",
    "        }\n",
    "        for u, f in zip(unique, freq)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BMI</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.097054</td>\n",
       "      <td>0.268651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BW</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.074169</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DBP</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.181430</td>\n",
       "      <td>0.264808</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SBP</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.181596</td>\n",
       "      <td>0.264728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lipid_Cholesterol</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.354286</td>\n",
       "      <td>0.319069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lipid_LDL</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.445036</td>\n",
       "      <td>0.356468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lipid_Triglyceride</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.340320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Chem_HbA1C</td>\n",
       "      <td>23429.0</td>\n",
       "      <td>0.577346</td>\n",
       "      <td>0.370588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                index    count      mean       std  min       25%       50%  \\\n",
       "0                 BMI  23429.0  0.097054  0.268651  0.0  0.000000  0.000000   \n",
       "1                  BW  23429.0  0.074169  0.238300  0.0  0.000000  0.000000   \n",
       "2                 age  23429.0  0.000000  0.000000  0.0  0.000000  0.000000   \n",
       "3                 DBP  23429.0  0.181430  0.264808  0.0  0.000000  0.083333   \n",
       "4                 SBP  23429.0  0.181596  0.264728  0.0  0.000000  0.083333   \n",
       "5   Lipid_Cholesterol  23429.0  0.354286  0.319069  0.0  0.076923  0.285714   \n",
       "6           Lipid_LDL  23429.0  0.445036  0.356468  0.0  0.142857  0.333333   \n",
       "7  Lipid_Triglyceride  23429.0  0.468600  0.340320  0.0  0.181818  0.416667   \n",
       "8          Chem_HbA1C  23429.0  0.577346  0.370588  0.0  0.230769  0.615385   \n",
       "\n",
       "        75%  max  \n",
       "0  0.000000  1.0  \n",
       "1  0.000000  1.0  \n",
       "2  0.000000  0.0  \n",
       "3  0.250000  1.0  \n",
       "4  0.250000  1.0  \n",
       "5  0.545455  1.0  \n",
       "6  0.750000  1.0  \n",
       "7  0.750000  1.0  \n",
       "8  1.000000  1.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# patient_missing_data, missing_data_desc = get_patient_missing_data(df)\n",
    "\n",
    "patient_missing_data = pd.read_csv(docs_path + 'missing_rate_longitudinal.csv')\n",
    "patient_missing_data = patient_missing_data[patient_missing_data['ENC_HN'].isin(patients)].reset_index(drop = True)\n",
    "missing_data_desc = patient_missing_data.describe().T.reset_index()\n",
    "missing_data_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_impute   = ['BMI', 'BW', 'age', 'DBP', 'SBP', 'Lipid_Cholesterol', 'Lipid_LDL','Lipid_Triglyceride', 'Chem_HbA1C']\n",
    "missing_patterns = {} \n",
    "proportion_denom = {} \n",
    "for patient in tqdm(df['ENC_HN'].unique().tolist()):\n",
    "    patient_df = df[df['ENC_HN'] == patient]\n",
    "    missing_pattern = generate_miss_pattern(patient_df, columns_impute, threshold = 0.01, top = False)\n",
    "    missing_patterns[patient] = missing_pattern\n",
    "    proportion_den = len(patient_df)\n",
    "    proportion_denom[patient] = proportion_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_df = df.dropna(subset = [col for col in df.columns if col not in columns_low_missing])\n",
    "complete_df = df.dropna().reset_index(drop = True)\n",
    "complete_df['flag'] = 1\n",
    "complete_df['count'] = complete_df.groupby('ENC_HN')['flag'].transform('sum')\n",
    "\n",
    "complete_df = complete_df[complete_df['count'] >= 5].reset_index(drop = True)\n",
    "complete_df = complete_df.drop(columns = ['flag', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ENC_HN                0.0\n",
       "modulo_365            0.0\n",
       "BMI                   0.0\n",
       "BW                    0.0\n",
       "age                   0.0\n",
       "DBP                   0.0\n",
       "SBP                   0.0\n",
       "Lipid_Cholesterol     0.0\n",
       "Lipid_LDL             0.0\n",
       "Lipid_Triglyceride    0.0\n",
       "Chem_HbA1C            0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df.isnull().sum()/complete_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ampute_dataset(da, missing_pattern, columns_ampute):\n",
    "    seed = 100\n",
    "    df_list = [] \n",
    "\n",
    "    for patient in da['ENC_HN'].unique().tolist():\n",
    "        complete = da[da['ENC_HN'] == patient][columns_ampute]\n",
    "\n",
    "        pattern = missing_pattern.get(patient)\n",
    "        pattern = [d for d in pattern if d['incomplete_vars']]\n",
    "        if not pattern:\n",
    "            continue\n",
    "\n",
    "        total_freq = sum(d['freq'] for d in pattern)\n",
    "        if total_freq != 1:\n",
    "            remaining_diff = 1 - total_freq\n",
    "            pattern[-1]['freq'] += remaining_diff \n",
    "\n",
    "        len_patient_data = proportion_denom[patient]\n",
    "        proportion = (len_patient_data - len(complete)) / len_patient_data\n",
    "        amputer = MultivariateAmputation(prop = proportion, seed = seed, patterns = pattern)\n",
    "        incomplete = amputer.fit_transform(complete)\n",
    "        incomplete_df = pd.DataFrame(incomplete, columns = columns_ampute)\n",
    "        incomplete_df['ENC_HN'] = patient\n",
    "        df_list.append(incomplete_df)\n",
    "    incomplete_data = pd.concat(df_list, axis = 0)\n",
    "    return incomplete_data, amputer\n",
    "\n",
    "columns_ampute   = ['BMI', 'BW', 'age', 'DBP', 'SBP', 'Lipid_Cholesterol', 'Lipid_LDL','Lipid_Triglyceride', 'Chem_HbA1C']\n",
    "# try:\n",
    "#     ampute_data, amputer = ampute_dataset(complete_df, missing_patterns, columns_ampute)\n",
    "# except ValueError as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "\n",
    "incomplete_data, amputer = ampute_dataset(complete_df, missing_patterns, columns_ampute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_matrix(data):\n",
    "    if data.shape[0] < 2:\n",
    "        return np.eye(1)  \n",
    "    \n",
    "    X_t = data[:-1]\n",
    "    X_t_plus_1 = data[1:]\n",
    "    A, _, _, _ = np.linalg.lstsq(X_t, X_t_plus_1, rcond=None)\n",
    "    return A.reshape(1, 1)\n",
    "\n",
    "def impute(data, n_datasets, variable_schema, truncation, time_variable, \n",
    "           cov_threshold = 0.5, id_col = 'ENC_HN', n_iteration = 50, \n",
    "           convergence = ['maxit', 'delta', 'early_stop'], model='linear', \n",
    "           seed = 1997, verbose = True):\n",
    "    df_list, delta_change, convergence_data = [], [], []\n",
    "    for m in range(n_datasets):\n",
    "        if verbose:\n",
    "            print(f'Dataset number {m + 1}')\n",
    "\n",
    "        filled_df = data.copy()\n",
    "        mask = filled_df.isna()\n",
    "        imp_order = filled_df[list(variable_schema.keys())].isna().sum().sort_values(ascending=False).index\n",
    "\n",
    "        for to_impute in imp_order:\n",
    "            np.random.seed(seed)\n",
    "            local_mu = filled_df[to_impute].mean()\n",
    "            local_sd = filled_df[to_impute].std()\n",
    "            guesses = np.random.normal(loc   = local_mu, \n",
    "                                       scale = local_sd,\n",
    "                                       size  = len(filled_df))\n",
    "            filled_df.loc[mask[to_impute], to_impute] = guesses[mask[to_impute]]\n",
    "        it, delta = 0, []\n",
    "        while it < n_iteration:\n",
    "            if verbose:\n",
    "                print(f'Iteration {it + 1}', end = ', ')\n",
    "            pre_it = filled_df[imp_order].copy()\n",
    "\n",
    "            for patient_id, patient_data in filled_df.groupby(id_col):\n",
    "                for to_impute in imp_order:\n",
    "                    if to_impute in time_variable:\n",
    "                        covariates = patient_data[variable_schema[to_impute]].values\n",
    "                        observed_data = patient_data[to_impute].fillna(0).values.reshape(-1, 1)\n",
    "                        A = estimate_transition_matrix(observed_data)\n",
    "                        kf = KalmanFilter(transition_matrices = A,\n",
    "                                          transition_covariance = 0.1 * np.eye(A.shape[0]), observation_covariance = 1.0,\n",
    "                                          initial_state_mean  = [observed_data.mean()])\n",
    "                        state_means, state_covariances = kf.smooth(observed_data)\n",
    "\n",
    "                        reg = LinearRegression()\n",
    "                        X_train = np.hstack([state_means, covariates])\n",
    "\n",
    "                        reg.fit(X_train, observed_data.flatten())\n",
    "\n",
    "                        X_test = np.hstack([state_means, patient_data[variable_schema[to_impute]].values])\n",
    "                        y_pred = reg.predict(X_test)\n",
    "\n",
    "                        cov_diag = np.array([np.diag(cov) for cov in state_covariances]).flatten()\n",
    "                        low_cov_samples = cov_diag < cov_threshold\n",
    "                        valid_indices = patient_data.index[low_cov_samples]\n",
    "\n",
    "                        bounds = truncation[to_impute]\n",
    "                        y_pred = np.clip(y_pred, bounds[0], bounds[1])\n",
    "                        y_pred = winsorize(y_pred, limits = (0.10, 0.10))\n",
    "\n",
    "                        filled_df.loc[valid_indices, to_impute] = (\n",
    "                            filled_df.loc[valid_indices, to_impute]\n",
    "                            .where(~mask.loc[valid_indices, to_impute], y_pred[low_cov_samples]))\n",
    "                    else:\n",
    "                        sampled_data = pre_it[~mask[to_impute]]\n",
    "                        X_train = sampled_data[variable_schema[to_impute]]\n",
    "                        y_train = sampled_data[to_impute]\n",
    "\n",
    "                        estimator = BayesianRidge()\n",
    "                        estimator.fit(X_train, y_train)\n",
    "\n",
    "                        y_imputed = estimator.predict(filled_df[variable_schema[to_impute]])\n",
    "\n",
    "                        bounds = truncation[to_impute]\n",
    "                        y_imputed = np.clip(y_imputed, bounds[0], bounds[1])\n",
    "                        y_imputed = winsorize(y_imputed, limits=(0.10, 0.10))\n",
    "\n",
    "                        filled_df[to_impute] = filled_df[to_impute].where(~mask[to_impute], y_imputed)\n",
    "            post_it = filled_df[imp_order].copy()\n",
    "            delta_val = np.sqrt(((post_it - pre_it) ** 2).sum() / len(post_it))\n",
    "            delta.append(delta_val)\n",
    "\n",
    "            if np.nanmax(delta_val) < 1e-4:\n",
    "                break\n",
    "            it += 1\n",
    "        df_list.append(filled_df)\n",
    "        delta_change.append(pd.concat(delta, axis='columns'))\n",
    "        convergence_data.append(post_it)\n",
    "\n",
    "    result = {\n",
    "        'imputed_data': df_list,\n",
    "        'convergence_data': convergence_data,\n",
    "        'iteration_delta': delta_change}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variable_schema():\n",
    "    variable_schema = {\n",
    "        'BW': ['Lipid_Cholesterol', 'Lipid_LDL', 'Lipid_Triglyceride'],\n",
    "        'Lipid_LDL': ['Lipid_Cholesterol', 'Lipid_Triglyceride', 'BW'],\n",
    "        'Chem_HbA1C': ['Lipid_Cholesterol', 'Lipid_LDL', 'Lipid_Triglyceride'],\n",
    "        'Lipid_Cholesterol': ['Lipid_LDL', 'Lipid_Triglyceride', 'BW'],\n",
    "        'Lipid_Triglyceride': ['Lipid_Cholesterol', 'Lipid_LDL', 'BW'], }\n",
    "    return variable_schema\n",
    "\n",
    "def pool_imputed_data(imputed_data):\n",
    "    numeric_columns = imputed_data[0].select_dtypes(include=[np.number]).columns\n",
    "    stacked_data = np.stack([df[numeric_columns].values for df in imputed_data], axis=0)\n",
    "    pooled_mean = np.nanmean(stacked_data, axis=0)\n",
    "    pooled_df = pd.DataFrame(pooled_mean, columns=numeric_columns)\n",
    "    for col in imputed_data[0].columns:\n",
    "        if col not in numeric_columns:\n",
    "            pooled_df[col] = imputed_data[0][col]\n",
    "    return pooled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset number 1\n",
      "Iteration 1, Iteration 2, Iteration 3, Iteration 4, Iteration 5, Iteration 6, Iteration 7, Iteration 8, Iteration 9, Iteration 10, Dataset number 2\n",
      "Iteration 1, Iteration 2, Iteration 3, Iteration 4, Iteration 5, Iteration 6, Iteration 7, Iteration 8, Iteration 9, Iteration 10, Dataset number 3\n",
      "Iteration 1, Iteration 2, Iteration 3, Iteration 4, Iteration 5, Iteration 6, Iteration 7, Iteration 8, Iteration 9, Iteration 10, Dataset number 4\n",
      "Iteration 1, Iteration 2, Iteration 3, Iteration 4, Iteration 5, Iteration 6, Iteration 7, Iteration 8, Iteration 9, Iteration 10, Dataset number 5\n",
      "Iteration 1, Iteration 2, Iteration 3, Iteration 4, Iteration 5, Iteration 6, Iteration 7, Iteration 8, Iteration 9, Iteration 10, "
     ]
    }
   ],
   "source": [
    "columns_complete = []\n",
    "imputed_data = impute(data = df, \n",
    "                      n_datasets = 5, \n",
    "                      variable_schema = init_variable_schema(), \n",
    "                      truncation = init_truncation(), \n",
    "                      n_iteration = 10, \n",
    "                      convergence = 'early_stop',\n",
    "                      seed = 1996, \n",
    "                      model = 'linear', \n",
    "                      time_variable = columns_impute,\n",
    "                      verbose = True)\n",
    "\n",
    "imputed_data_list = imputed_data['imputed_data'] \n",
    "pooled_result = pool_imputed_data(imputed_data_list)\n",
    "pooled_result = pooled_result[['ENC_HN','modulo_365', 'BW', 'age', 'Lipid_Cholesterol', 'Lipid_LDL', 'Lipid_Triglyceride', 'Chem_HbA1C']]\n",
    "complete_dataset = pooled_result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_patterns = {} \n",
    "for patient in df['ENC_HN'].unique().tolist():\n",
    "    patient_df = df[df['ENC_HN'] == patient]\n",
    "    missing_pattern = generate_miss_pattern(patient_df, columns_impute, threshold = 0.15, top = False)\n",
    "    missing_patterns[patient] = missing_pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
