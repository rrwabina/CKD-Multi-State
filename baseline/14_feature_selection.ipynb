{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import pyampute\n",
    "import pickle \n",
    "import time\n",
    "import ast \n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import tree\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tableone import TableOne \n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from app_transition_dict import get_transition_dict, get_transition_code\n",
    "from app_init import get_multi_state_covariates, get_multi_state_cov_quartiles\n",
    "from app_init import replace_covariate_labels, replace_pvalue, get_variables_cox\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "drive = 'G'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/data/CKD_COHORT_Jan2010_Mar2024_v3.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "resu_path = drive + ':/Shared drives/CKD_Progression/result/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'\n",
    "\n",
    "def generate_df_continuous(df, variables, q = 3):\n",
    "    bins_dict = {} \n",
    "    for variable, prefix, column_name in variables:\n",
    "        df[column_name], bins = pd.qcut(\n",
    "            df[variable],\n",
    "            q = q,\n",
    "            labels = False,\n",
    "            duplicates = 'drop',\n",
    "            retbins = True)\n",
    "        bins_dict[variable] = bins\n",
    "        dummies = pd.get_dummies(df[column_name], prefix = prefix)\n",
    "        df = pd.concat([df, dummies], axis = 1)\n",
    "    return df, bins_dict\n",
    "\n",
    "def generate_df_continuous_predefined(df, variables, get_columns = False):\n",
    "    for variable, prefix, column_name, bins, labels in variables:\n",
    "        df[column_name] = pd.cut(df[variable], bins = bins, labels = labels, right = False)\n",
    "\n",
    "        if get_columns:\n",
    "            dummies = pd.get_dummies(df[column_name], prefix = prefix)\n",
    "            df = pd.concat([df, dummies], axis = 1)\n",
    "    return df\n",
    "\n",
    "def get_first_dates():\n",
    "    heart_failure = pd.read_excel(docs_path + 'HF_FIRSTDATE_2010_2023.xlsx') ['ENC_HN'].unique().tolist()\n",
    "    hypertension  = pd.read_excel(docs_path + 'HTN_FIRSTDATE_2010_2023.xlsx')['ENC_HN'].unique().tolist()\n",
    "    diabetes = pd.read_csv(docs_path + 'DM_FIRSTDATE_2010_2023.csv')['ENC_HN'].unique().tolist()\n",
    "    atrialfb = pd.read_csv(docs_path + 'AF_FIRSTDATE_2010_2023.csv')['ENC_HN'].unique().tolist()\n",
    "    return heart_failure, hypertension, diabetes, atrialfb\n",
    "\n",
    "def merge_comorbidity(df, comorbidity, disease_code):\n",
    "    disease_column = disease_code.upper()\n",
    "    df[disease_column] = df['ENC_HN'].isin(comorbidity).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(version = '13', get_columns = False):\n",
    "    covariates, variables = get_multi_state_cov_quartiles(), get_variables_cox()\n",
    "    order_covariates = pd.read_csv(docs_path + 'cox_covariates.csv')\n",
    "    model_vars = ['ENC_HN', 'transition', 'fr', 'to', 'status', 'tstart', 'tstops', 'time']\n",
    "    heart_failure, hypertension, diabetes, atrialfb = get_first_dates()\n",
    "    \n",
    "    long_df = pd.read_csv(save_path + 'multi_state_long_ver0' + f'{version}.csv')\n",
    "    long_df['gender']  = long_df['gender'].replace('M', 1).replace('F', 0)\n",
    "    long_df['pathway'] = long_df['fr'] + '_to_' + long_df['to']\n",
    "    long_df = generate_df_continuous_predefined(long_df, variables, get_columns = get_columns)\n",
    "    long_df['statin']  = long_df[['statinhydro', 'statinlipo']].max(axis = 1)\n",
    "    long_df['raas']    = long_df[['arb', 'acei']].max(axis = 1)\n",
    "    long_df = long_df.drop(columns = ['statinhydro', 'statinlipo'])\n",
    "    long_df = merge_comorbidity(long_df, heart_failure, 'hf')\n",
    "    long_df = merge_comorbidity(long_df, diabetes,      'dm')\n",
    "    long_df = merge_comorbidity(long_df, atrialfb,      'af')\n",
    "    return covariates, order_covariates, long_df\n",
    "\n",
    "\n",
    "covariates, order_covariates, long_df = load_dataset(version = '13', get_columns = False)\n",
    "covariates.remove('Gout')\n",
    "covariates.remove('PHOS_BINDER')\n",
    "covariates.remove('ANTI_PL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_coxph(long_df, covariate, pathway):\n",
    "    transition_data = long_df[long_df['pathway'] == pathway].copy()\n",
    "    cph = CoxPHFitter()\n",
    "    formula = f'C({covariate})'\n",
    "    cph_model = cph.fit(\n",
    "        transition_data,\n",
    "        duration_col = 'time',\n",
    "        event_col    = 'status',\n",
    "        formula = formula,\n",
    "        step_size = 0.1, \n",
    "        show_progress = False)\n",
    "    return cph_model\n",
    "\n",
    "def coxph_statistics(result):\n",
    "    roundup = 2\n",
    "    hazd = np.round(result.hazard_ratios_, roundup)[0]\n",
    "    ster = np.round(result.standard_errors_[0], roundup)\n",
    "    coef_low, coef_upp  = result.confidence_intervals_.reset_index().loc[0, '95% lower-bound'], \\\n",
    "                          result.confidence_intervals_.reset_index().loc[0, '95% upper-bound']\n",
    "    confidence_interval = f'({np.round(np.exp(coef_low), roundup)}, {np.round(np.exp(coef_upp), roundup)})'\n",
    "    pvalue = np.round(result._compute_p_values(), roundup)[0]\n",
    "    events_obs = result.event_observed.sum()\n",
    "    events_tot = result.event_observed.shape[0] \n",
    "    return hazd, confidence_interval, pvalue, ster, events_obs, events_tot\n",
    "\n",
    "def get_log_likelihood(cph_model):\n",
    "    deg = len(cph_model.params_)\n",
    "    LL0 = cph_model._ll_null_\n",
    "    LL1 = cph_model.log_likelihood_\n",
    "    LLR = -2 * (LL0 - LL1)\n",
    "    p_value = chi2.sf(LLR, len(cph_model.params_))\n",
    "    return deg, LL0, LL1, LLR, p_value\n",
    "\n",
    "def cox_table(cph_model, variable):\n",
    "    categorical = ['bin_bmi', 'bin_glu', 'bin_hba']\n",
    "    test_statistics = cph_model.log_likelihood_ratio_test().test_statistic\n",
    "    univariate_covariate = cph_model.summary.reset_index()\n",
    "    univariate_covariate['95CI HR'] = '(' + np.round(univariate_covariate['exp(coef) lower 95%'], 2).astype(str) + ', ' +\\\n",
    "                                            np.round(univariate_covariate['exp(coef) upper 95%'], 2).astype(str) + ')'\n",
    "    univariate_covariate = univariate_covariate.drop(columns = ['se(coef)', 'z',\n",
    "                                                                'coef lower 95%', \n",
    "                                                                'coef upper 95%', \n",
    "                                                                'exp(coef) lower 95%', \n",
    "                                                                'exp(coef) upper 95%',])\n",
    "    univariate_covariate = univariate_covariate.rename(columns = {'exp(coef)': 'HR', 'p': 'pvalue'})\n",
    "    univariate_covariate = univariate_covariate[['covariate', 'coef', 'HR', '95CI HR', 'pvalue']]\n",
    "    univariate_covariate['LLT'] = test_statistics\n",
    "    univariate_covariate['deg'] = len(cph_model.params_)\n",
    "    univariate_covariate = pd.concat([pd.DataFrame({'covariate': [variable], 'coef': [0], 'HR': [1], '95CI HR': np.NaN, 'pvalue': np.NaN}), \n",
    "                                    univariate_covariate], \n",
    "                                    ignore_index = True)\n",
    "    if variable in categorical:\n",
    "        deg, LL0, LL1, LLR, p_value = get_log_likelihood(cph_model)\n",
    "        univariate_covariate.loc[variable, 'LLT'] = LLR\n",
    "        univariate_covariate.loc[variable, 'deg'] = deg\n",
    "        univariate_covariate.loc[variable, 'pvalue'] = p_value\n",
    "    return univariate_covariate\n",
    "\n",
    "\n",
    "def univariate_coxph(df, pathway, save = False):\n",
    "    univariate_list = []\n",
    "    categorical = ['bin_bmi', 'bin_glu', 'bin_hba']\n",
    "    for covariate in tqdm(covariates):\n",
    "        cph_model = generate_coxph(df, covariate, pathway)\n",
    "        univariate_covariate = cox_table(cph_model, covariate)\n",
    "        univariate_covariate['percentage'] = df[df[covariate] == 1]['ENC_HN'].nunique()/df['ENC_HN'].nunique()\n",
    "        univariate_list.append(univariate_covariate)\n",
    "    univariate_df = pd.concat(univariate_list, axis = 0)\n",
    "    univariate_df = pd.merge(univariate_df, order_covariates, on = 'covariate', how = 'inner')\n",
    "    univariate_df = univariate_df[univariate_df['include'] == 1]\n",
    "    univariate_df = univariate_df.sort_values(['order'], ascending = True)\n",
    "    univariate_df['covariate'] = univariate_df['replace']\n",
    "    univariate_df['patient_observed'] = cph_model.event_observed.sum()\n",
    "    univariate_df['patient_risk'] = df['ENC_HN'].nunique()\n",
    "    univariate_df = univariate_df[['variable', 'covariate', 'patient_observed', 'patient_risk', 'LLT', 'deg', 'percentage', 'coef', 'HR', '95CI HR', 'pvalue']]\n",
    "    for var in ['coef', 'HR', 'pvalue', 'LLT', 'percentage']:\n",
    "        univariate_df[var] = np.round(univariate_df[var], 2)\n",
    "    univariate_df = replace_pvalue(univariate_df)\n",
    "    univariate_df['included'] = np.NaN\n",
    "    univariate_df['pvalue'] = chi2.sf(univariate_df['LLT'], univariate_df['deg'])\n",
    "    return univariate_df, cph_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3A_to_CKD3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:56<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3A_to_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:50<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3A_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:04<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_to_CKD3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:32<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_to_CKD4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:43<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_to_CKD5A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:00<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_to_CKD5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:39<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVD_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:37<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3B_to_CKD4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:14<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3B_to_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD3B_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:38<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD4_to_CKD5A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:45<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD4_to_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:32<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD4_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:23<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD5A_to_CKD5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:36<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD5A_to_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD5A_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:17<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD5B_to_CVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:27<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CKD5B_to_DEAD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "pathways = long_df['pathway'].unique().tolist()\n",
    "for path in ['CKD3A_to_CKD4', 'CKD3A_to_CKD5A', 'CKD3A_to_CKD5B', 'CKD3B_to_CKD5A', 'CKD3B_to_CKD5B', 'CKD4_to_CKD5B']:\n",
    "    pathways.remove(path)\n",
    "\n",
    "for pathway in pathways:\n",
    "    print(pathway)\n",
    "    univariate, cph_model = univariate_coxph(long_df, pathway, save = False)\n",
    "    univariate['variable'] = univariate['variable'].replace('hdl_low ', 'hdl_low').replace('rua_normal ', 'rua_normal')\n",
    "    univariate.to_csv(resu_path + f'univariate/feature_selection/{pathway}.csv', index = False)\n",
    "\n",
    "covariates, order_covariates, long_df = load_dataset(version = '13', get_columns = True)\n",
    "for pathway in pathways:\n",
    "    df = long_df[long_df['pathway'] == pathway]\n",
    "    univariate = pd.read_csv(resu_path + f'univariate/feature_selection/{pathway}.csv')\n",
    "    percentage_columns = univariate['variable'].tolist()\n",
    "    univariate['percentage'] = univariate.apply(\n",
    "        lambda row: df[df[row['variable']] == 1]['ENC_HN'].nunique() / df['ENC_HN'].nunique() * 100, axis = 1)\n",
    "    univariate.to_csv(resu_path + f'univariate/feature_selection/{pathway}.csv', index = False)\n",
    "\n",
    "path_list = [\n",
    "    os.path.join(os.path.join(resu_path, 'univariate/'), filename)\n",
    "    for filename in os.listdir(os.path.join(resu_path, 'univariate/feature_selection'))\n",
    "    if filename.endswith(('.csv', '.xls', '.xlsx'))]\n",
    "\n",
    "output_excel = resu_path + 'univariate/feature_selection/univariate_results.xlsx'\n",
    "with pd.ExcelWriter(output_excel, engine = 'openpyxl') as writer:\n",
    "    for file_path in path_list:\n",
    "        sheet_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.to_excel(writer, sheet_name = sheet_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates, order_covariates, long_df = load_dataset(version = '13', get_columns = True)\n",
    "for pathway in pathways:\n",
    "    df = long_df[long_df['pathway'] == pathway]\n",
    "    univariate = pd.read_csv(resu_path + f'univariate/LR_test/{pathway}.csv')\n",
    "    percentage_columns = univariate['variable'].tolist()\n",
    "    univariate['percentage'] = univariate.apply(\n",
    "        lambda row: df[df[row['variable']] == 1]['ENC_HN'].nunique() / df['ENC_HN'].nunique() * 100, axis = 1)\n",
    "    univariate.to_csv(resu_path + f'univariate/LR_test/{pathway}.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
