{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N o t e :', 'T h is', 't es t<END>', 'is', 't o', 'g a u g e<END>', 'y ou r<END>', 'k n o w l e d g e<END>', 'in', 'P y th on', 'p r o g r a m m in g', 'a n d<END>', 'N L P', 'a r c h i t e c t u r e<END>', 'a n d<END>', 'th e o r y .', 'I t<END>', 'is', 'e x p e c t e d<END>', 'th a t<END>', 'n o', 'A I', 'o r<END>', 'I n t e r n e t<END>', 'a s s is t a n c e<END>', 'w i l l', 'b e<END>', 'u s e d<END>', 't o', 'a n s w e r<END>', 'q u es t i on s', 'in', 'th is', 't es t .', 'B y', 'c o m p l e t in g', 'th is', 't es t ,', 'y ou', 's w e a r<END>', 'th a t<END>', 'y ou', 'h a v e<END>', 'n o t<END>', 'u s e d<END>', 'A I', 'a s s is t a n c e<END>', 'a n d<END>', 'h a v e<END>', 'p r o v i d e d<END>', 'y ou r<END>', 'o w n ,', 'u n a i d e d ,', 'r es p on s es .', 'I f', 'd is c o v e r e d<END>', 'th a t<END>', 'y ou', 'h a v e<END>', 'u s e d<END>', 'A I', 'a t<END>', 'a n y', 'p o in t<END>', 'd u r in g', 'th e<END>', 'r e c r u i t m e n t<END>', 'p r o c es s ,', 'y ou', 'w i l l', 'n o t<END>', 'b e<END>', 'c on s i d e r e d<END>', 'f o r<END>', 'th e<END>', 'p o s i t i on .', 'G i v e<END>', 'y ou r<END>', 'h on es t<END>', 'o p in i on', 'on', 'th e<END>', 'q u es t i on s ,', 'w r on g', 'a n s w e r s', 'w i l l', 'n o t<END>', 'b e<END>', 'd is q u a l i f y in g .', 'Y ou', 'm a y', 'u s e<END>', 'th e<END>', 'in t e r n e t ,', 'b u t<END>', 'n o t<END>', 'A I ,', 'f o r<END>', 'q u es t i on s', '4 .']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "def get_pair_stats(word_freq):\n",
    "    '''\n",
    "    Count frequency of adjacent symbol pairs across all words.\n",
    "    Args: \n",
    "        word_freq (dict): A dictionary of words and their frequencies.\n",
    "    \n",
    "    Returns: \n",
    "        dict: A dictionary of symbol pairs and their frequencies.\n",
    "    '''\n",
    "    pair_stats = {}\n",
    "    for symbols, freq in word_freq.items():\n",
    "        for i in range(len(symbols) - 1):   \n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_stats[pair] = pair_stats.get(pair, 0) + freq\n",
    "    return pair_stats \n",
    "\n",
    "def merge_pair_in_words(best_pair, word_freq):\n",
    "    '''\n",
    "    Merge the best_pair in every word in word_freq.\n",
    "    If best_pair = ('a', 'n'), then in the word ('b', 'a', 'n', 'a', '</w>'),\n",
    "        we merge occurrences of adjacent 'a' 'n' into ('an').\n",
    "\n",
    "    Args:   best_pair (tuple): The pair of symbols to merge.\n",
    "            word_freq (dict): A dictionary of words and their frequencies.\n",
    "    Returns: dict: A dictionary of merged words and their frequencies.\n",
    "    '''\n",
    "    merged_word_freq = {}\n",
    "    bigram = best_pair\n",
    "    for symbols, freq in word_freq.items():\n",
    "        new_symbols = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            if (i < len(symbols) - 1) and (symbols[i], symbols[i+1]) == bigram:\n",
    "                new_symbols.append(symbols[i] + symbols[i+1])  # e.g. 'a'+'n' -> 'an'\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_symbols = tuple(new_symbols)\n",
    "        merged_word_freq[new_symbols] = merged_word_freq.get(new_symbols, 0) + freq\n",
    "    return merged_word_freq\n",
    "\n",
    "def apply_bpe_to_word(word, merges):\n",
    "    ''' \n",
    "    Applies the learned merges to a single word (tuple of chars) in sequential order.\n",
    "        e.g., merges = [(('a','n'), 'an'), (('an','a'), 'ana'), ...].\n",
    "        We keep merging until no merge can be applied or we've exhausted merges.\n",
    "\n",
    "    Args:   word (tuple): The word to tokenize as a tuple of characters.\n",
    "            merges (list): A list of merges learned from the training data.\n",
    "\n",
    "    Returns: list: A list of subword units after applying BPE merges\n",
    "    '''\n",
    "    symbols = list(word)\n",
    "    i = 0\n",
    "    for (bigram, merged_symbol) in merges:\n",
    "        i = 0\n",
    "        while i < (len(symbols) - 1):\n",
    "            if (symbols[i], symbols[i+1]) == bigram:\n",
    "                symbols[i] = merged_symbol\n",
    "                del symbols[i+1]  \n",
    "            else:\n",
    "                i += 1\n",
    "    return symbols\n",
    "\n",
    "\n",
    "def tokenize_bpe(text, num_merges = 5, special_end_token = '<END>'):\n",
    "    ''' \n",
    "    Tokenize an input text using a more realistic (less naive) Byte-Pair Encoding (BPE).\n",
    "    This function both learns and applies the BPE merges from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text (can be multiple sentences) to tokenize.\n",
    "        num_merges (int): The maximum number of BPE merges to learn.\n",
    "        special_end_token (str): A marker to indicate the end of a word.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of BPE-tokenized words (subwords)\n",
    "    '''\n",
    "    words = re.findall(r'\\S+', text)\n",
    "    word_freq = {}\n",
    "    \n",
    "    for w in words:\n",
    "        chars = tuple(list(w) + [special_end_token])\n",
    "        word_freq[chars] = word_freq.get(chars, 0) + 1\n",
    "\n",
    "    merges = []  \n",
    "    for _ in range(num_merges):\n",
    "        pair_stats = get_pair_stats(word_freq)\n",
    "        if not pair_stats:\n",
    "            break\n",
    "        best_pair = max(pair_stats, key = pair_stats.get)\n",
    "        best_pair_count = pair_stats[best_pair]\n",
    "\n",
    "        if best_pair_count == 0:\n",
    "            break\n",
    "        merges.append((best_pair, ''.join(best_pair)))\n",
    "        word_freq = merge_pair_in_words(best_pair, word_freq)\n",
    "    tokenized_text = []\n",
    "    for w in words:\n",
    "        chars = tuple(list(w) + [special_end_token])\n",
    "        subword_units = apply_bpe_to_word(chars, merges)\n",
    "        if subword_units and subword_units[-1] == special_end_token:\n",
    "            subword_units = subword_units[:-1]\n",
    "        tokenized_text.append(' '.join(subword_units)) \n",
    "    return tokenized_text\n",
    "\n",
    "sample_text = ''' \n",
    "                Note: This test is to gauge your knowledge in Python programming and NLP architecture and theory. \n",
    "                It is expected that no AI or Internet assistance will be used to answer questions in this test. \n",
    "                By completing this test, you swear that you have not used AI assistance and have provided your own, unaided, responses. \n",
    "                If discovered that you have used AI at any point during the recruitment process, you will not be considered for the position. \n",
    "                Give your honest opinion on the questions, wrong answers will not be disqualifying.\n",
    "                You may use the internet, but not AI, for questions 4.\n",
    "              '''\n",
    "tokenized = tokenize_bpe(sample_text, num_merges = 10)\n",
    "print(tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
