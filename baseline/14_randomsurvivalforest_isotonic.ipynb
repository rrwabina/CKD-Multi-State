{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shap\n",
    "from scipy.stats import mstats\n",
    "from scipy.stats.mstats import winsorize\n",
    "from scipy import stats\n",
    "from pyampute.ampute import MultivariateAmputation\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from statsmodels.gam.tests.test_penalized import df_autos\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score)\n",
    "from sksurv.metrics import brier_score\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.preprocessing import OneHotEncoder, encode_categorical\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from data import load_dataset \n",
    "from sklearn.metrics import make_scorer\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "drive = 'H'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/data/CKD_COHORT_Jan2010_Mar2024_v3.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "resu_path = drive + ':/Shared drives/CKD_Progression/result/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'\n",
    "\n",
    "covariates, order_covariates, long_df = load_dataset(get_columns = True)\n",
    "\n",
    "def concordance_scorer(y_true, y_pred):\n",
    "    events = y_true['event']\n",
    "    times  = y_true['time']\n",
    "    return concordance_index_censored(events, times, y_pred)[0]\n",
    "\n",
    "def concordance_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return concordance_index_censored(y['status'], y['time'], y_pred)[0]\n",
    "\n",
    "def brier_score_scorer(estimator, X, y):\n",
    "    y_pred = estimator.predict_survival_function(X)\n",
    "    return brier_score(y['status'], y['time'], y_pred, time_point = 1)\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['variable'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "    return vif_data\n",
    "\n",
    "def calculate_nll_loss(model, X, y_true):\n",
    "    survival_probs = model.predict_survival_function(X)  \n",
    "    nll = 0  \n",
    "    for i, fn in enumerate(survival_probs):\n",
    "        time_at_event = y_true['time'][i]\n",
    "        if time_at_event <= 0:\n",
    "            continue \n",
    "        prob_of_survival = fn(time_at_event)\n",
    "        event_status = y_true['status'][i]\n",
    "        if event_status:\n",
    "            nll -= np.log(1 - prob_of_survival)\n",
    "        else:\n",
    "            nll -= np.log(prob_of_survival)\n",
    "    return nll / len(survival_probs) \n",
    "\n",
    "def calculate_median_survival_time(surv_func):\n",
    "    for t, prob in zip(surv_func.x, surv_func.y):\n",
    "        if prob <= 0.5:\n",
    "            return t\n",
    "    return surv_func.x[-1] \n",
    "\n",
    "def calculate_cindex(model, X, y_true):\n",
    "    surv_funcs = model.predict_survival_function(X) \n",
    "    times = y_true['time']\n",
    "    events = y_true['status']\n",
    "    predicted_survival_times = np.array([calculate_median_survival_time(fn) for fn in surv_funcs])\n",
    "    c_index = concordance_index_censored(events.astype(bool), times, predicted_survival_times)[0]\n",
    "    return c_index\n",
    "\n",
    "def permutation_importance_rsf(model, X, y, metric_func, n_repeats = 2, random_state = None):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    baseline_score = metric_func(model, X, y)  \n",
    "    scores = np.zeros((n_repeats, X.shape[1]))\n",
    "\n",
    "    for i in tqdm(range(X.shape[1])):  \n",
    "        X_permuted = X.copy()\n",
    "        for n in range(n_repeats):  \n",
    "            X_permuted[:, i] = rng.permutation(X[:, i])  \n",
    "            permuted_score = metric_func(model, X_permuted, y)  \n",
    "            scores[n, i] = permuted_score - baseline_score  \n",
    "    importances_mean = np.mean(scores, axis = 0)\n",
    "    importances_std  = np.std(scores,  axis = 0)\n",
    "    return importances_mean, importances_std\n",
    "\n",
    "concordance_scorer = make_scorer(concordance_scorer, greater_is_better = True)\n",
    "brier_score_scorer = make_scorer(brier_score_scorer, greater_is_better = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = resu_path + 'modeling/randomsurvivalforest/rsf_results_log_significance.csv'\n",
    "get_variance_inflation, get_interaction = False, False\n",
    "if not os.path.exists(log_file):\n",
    "    with open(log_file, mode = 'w', newline = '') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['pathway', 'best_params', 'fit_predict_time',  'covariates', 'num_covariates',\n",
    "                         'survival_auc', 'survival_mauc', 'c_index', 'brier'])\n",
    "        \n",
    "pathways = long_df['pathway'].unique().tolist()\n",
    "for path in ['CKD3A_to_DEAD', 'CKD3A_to_CKD4', 'CKD3A_to_CKD5A', 'CKD3A_to_CKD5B', 'CKD3B_to_CKD5A', 'CKD3B_to_CKD5B', 'CKD4_to_CKD5B']:\n",
    "    pathways.remove(path)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "multicollinear_threshold = 10\n",
    "for pathway in tqdm(pathways):\n",
    "    print(f'Processing pathway: {pathway}')\n",
    "    if   'CKD' in pathway.split('_')[0] and 'CKD' in pathway.split('_')[2]:\n",
    "        sheet_pathway = 'CKD_to_CKD'\n",
    "    elif 'CKD' in pathway.split('_')[0] and 'CVD' in pathway.split('_')[2]:\n",
    "        sheet_pathway = 'CKD_to_CVD'\n",
    "    else:\n",
    "        sheet_pathway = 'CKD_to_CKD'\n",
    "\n",
    "    transition_df = long_df[long_df['pathway'] == pathway]\n",
    "\n",
    "    univariate  = pd.read_excel(resu_path + 'univariate/univariate_ML.xlsx', sheet_name = pathway)\n",
    "    interaction = pd.read_excel(resu_path + 'univariate/univariate_interaction.xlsx', sheet_name = sheet_pathway)\n",
    "    multivariate_covariates = univariate[univariate['included'] == 1]['variable'].tolist()\n",
    "    multivariate_primary    = univariate[univariate['included'] == 1]['variable'].tolist()\n",
    "\n",
    "    if get_variance_inflation:\n",
    "        if get_interaction:\n",
    "            multivariate_covariates_set = set(multivariate_covariates)\n",
    "            univariate_predictors = list(set(interaction['predictor1'].unique().tolist() + interaction['predictor2'].unique().tolist()))\n",
    "            for index, row in interaction.iterrows():\n",
    "                interaction_name = row['variable']\n",
    "                predictor1 = row['predictor1']\n",
    "                predictor2 = row['predictor2']\n",
    "                \n",
    "                transition_df[interaction_name] = transition_df[predictor1] * transition_df[predictor2]\n",
    "                multivariate_covariates_set.discard(predictor1)\n",
    "                multivariate_covariates_set.discard(predictor2)\n",
    "            multivariate_covariates_set.update(interaction['variable'].tolist())\n",
    "            multivariate_covariates = list(multivariate_covariates_set)\n",
    "\n",
    "        multivariate_covariates = list(set(multivariate_primary + multivariate_covariates))\n",
    "    X = transition_df[multivariate_covariates]\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns = multivariate_covariates)\n",
    "    vif_result = calculate_vif(X_scaled_df)\n",
    "    vif_result.to_csv(resu_path + f'modeling/randomsurvivalforest/variance_inflation/{pathway}_TODEL.csv')\n",
    "    multicollinear = vif_result[vif_result['VIF'] >= multicollinear_threshold]['variable'].tolist()\n",
    "    multivariate_covariates = vif_result[vif_result['VIF'] < multicollinear_threshold]['variable'].tolist()\n",
    "    print(f'There are {len(multicollinear)} covariates with high multicollinearity.')\n",
    "\n",
    "    independent_df = transition_df.drop(columns = ['status'])\n",
    "    independent_df = transition_df[multivariate_covariates]\n",
    "    groundtruth_df = transition_df[['status', 'time']]\n",
    "    groundtruth_df['status'] = groundtruth_df['status'].astype(bool)\n",
    "    groundtruth_df['time'] = pd.to_numeric(groundtruth_df['time'])\n",
    "\n",
    "    censored_weight = len(groundtruth_df) / groundtruth_df['status'].value_counts()[0]\n",
    "    event_weight    = len(groundtruth_df) / groundtruth_df['status'].value_counts()[1]\n",
    "\n",
    "    groundtruth_df['sample_weight'] = groundtruth_df['status'].apply(lambda x: event_weight if x else censored_weight)\n",
    "\n",
    "    X, y = independent_df, Surv.from_dataframe('status', 'time', groundtruth_df)\n",
    "    X_train, X_test, y_train, y_test, sample_weight_train, sample_weight_test = train_test_split(X, y, groundtruth_df['sample_weight'].to_numpy(), test_size = 0.30, random_state = 42)\n",
    "    X_train = scaler.fit_transform(X_train) \n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 300, 500],\n",
    "        'max_depth': [3, 5, None],\n",
    "        'min_samples_split': [2, 3],\n",
    "        'min_samples_leaf': [1, 2],\n",
    "        'max_features': ['sqrt', 0.5, 0.75],\n",
    "        'bootstrap': [True]}\n",
    "\n",
    "    start = time()\n",
    "    cv = StratifiedKFold(n_splits = 5, shuffle = True,  random_state = 42)\n",
    "    random_forest = RandomSurvivalForest(n_jobs = -1,   random_state = 42)\n",
    "    pipeline = Pipeline([('scaler', scaler), ('rsf', random_forest)])\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(y_train['time'], event_observed = 1 - y_train['status'])  \n",
    "    ipcw_weights = 1 / kmf.survival_function_at_times(y_train['time']).values.flatten()\n",
    "\n",
    "    survival_forest = RandomizedSearchCV(random_forest,\n",
    "                                         param_grid,\n",
    "                                         cv = cv,\n",
    "                                         n_iter = 2,\n",
    "                                         scoring = 'neg_log_loss')\n",
    "\n",
    "    survival_forest.fit(X_train, y_train, sample_weight = sample_weight_train)\n",
    "    best_survival = survival_forest.best_estimator_\n",
    "    fit_predict_time = np.round(time() - start, 3)\n",
    "\n",
    "    period_max = 100\n",
    "    times = np.arange(y_test['time'].min(), period_max, 12)\n",
    "    risk_score = best_survival.predict(X_test) \n",
    "    surv_preds = best_survival.predict_survival_function(X_test)\n",
    "    prediction = [fn(period_max) for fn in surv_preds]\n",
    "\n",
    "    survival_auc, survival_mauc = cumulative_dynamic_auc(y_train, y_test,  risk_score, times)\n",
    "    c_index = concordance_index_censored(y_test['status'], y_test['time'], risk_score)[0] \n",
    "    _, brier_score_val = brier_score(y_train, y_test, prediction, period_max)\n",
    "\n",
    "    with open(log_file, mode = 'a', newline = '') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([pathway,\n",
    "                         best_survival.get_params(),\n",
    "                         fit_predict_time,\n",
    "                         multivariate_covariates,\n",
    "                         len(multivariate_covariates),\n",
    "                         survival_auc,\n",
    "                         survival_mauc,\n",
    "                         c_index,\n",
    "                         brier_score_val[0]])\n",
    "    print(f'\\tFit and predict time of RSF at transition {pathway}: {fit_predict_time} seconds with {len(multivariate_covariates)} covariates.')\n",
    "    print(f'\\tC-index: {np.round(c_index, 2)} \\tBrier score: {np.round(brier_score_val[0], 2)}')\n",
    "    break\n",
    "    importances_mean, importances_std = permutation_importance_rsf(best_survival,\n",
    "                                                                    X_test.values, y_test,\n",
    "                                                                    calculate_cindex,  \n",
    "                                                                    n_repeats = 3, \n",
    "                                                                    random_state = 42)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_test.columns, 'importance_mean': importances_mean, 'importance_std' : importances_std})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by = 'importance_mean', ascending = False)\n",
    "    feature_importance_df.to_csv(resu_path + f'modeling/randomsurvivalforest/feature_importance/{pathway}_TODEL.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
