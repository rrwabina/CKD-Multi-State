{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score, mean_absolute_percentage_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxPH\n",
    "from pycox.models.loss import CoxPHLoss\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score)\n",
    "from sksurv.metrics import brier_score\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.preprocessing import OneHotEncoder, encode_categorical\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from data import load_dataset \n",
    "from sklearn.metrics import make_scorer\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from pycox.models import DeepHitSingle\n",
    "import shap\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "\n",
    "drive = 'G'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/data/CKD_COHORT_Jan2010_Mar2024_v3.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "resu_path = drive + ':/Shared drives/CKD_Progression/result/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'\n",
    "\n",
    "covariates, order_covariates, long_df = load_dataset(get_columns = True)\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pycox.models import CoxPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(pathway):\n",
    "    transition_df = long_df[long_df['pathway'] == pathway]\n",
    "    univariate  = pd.read_excel(resu_path + f'univariate/LR_test/{pathway}.xlsx', sheet_name = pathway)\n",
    "    multivariate_covariates = univariate[univariate['pvalue'] < 0.20]['variable'].tolist()\n",
    "    transition_df = transition_df[multivariate_covariates + ['time', 'status']]\n",
    "    df_train = transition_df.copy()\n",
    "    df_tests = df_train.sample(frac = 0.2, random_state=42)\n",
    "    df_train = df_train.drop(df_tests.index)\n",
    "    df_valid = df_train.sample(frac = 0.2, random_state=42)\n",
    "    df_train = df_train.drop(df_valid.index)\n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in multivariate_covariates]\n",
    "    x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "    X_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    X_valid = x_mapper.transform(df_valid).astype('float32')\n",
    "    X_tests = x_mapper.transform(df_tests).astype('float32')\n",
    "\n",
    "    num_durations = 10\n",
    "    labtrans = DeepHitSingle.label_transform(num_durations)\n",
    "    \n",
    "    get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "    y_train = labtrans.fit_transform(*get_target(df_train))\n",
    "    y_valid = labtrans.transform(*get_target(df_valid))\n",
    "    durations_test, events_test = get_target(df_tests)\n",
    "    val = X_valid, y_valid\n",
    "    \n",
    "    train = (X_train, y_train)\n",
    "    val   = (X_valid, y_valid)\n",
    "    durations_test, events_test = get_target(df_tests)\n",
    "    return X_train, y_train, val, X_tests, durations_test, events_test, df_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ace(surv, durations, events, time_grid):\n",
    "    ace = 0\n",
    "    n_bins = len(time_grid)\n",
    "    for t in time_grid:\n",
    "        nearest_time_index = surv.index.get_loc(t, method='nearest')\n",
    "        pred_surv = surv.iloc[nearest_time_index].values\n",
    "    \n",
    "        observed = (durations >= t).astype(int)\n",
    "        observed_censored = observed[events == 1]\n",
    "        pred_surv_censored = pred_surv[events == 1]\n",
    "        bin_error = np.abs(pred_surv_censored.mean() - observed_censored.mean())\n",
    "        ace += bin_error\n",
    "    \n",
    "    return ace / n_bins\n",
    "\n",
    "def conformal_coverage(surv, durations, events, alpha):\n",
    "    final_time_index = surv.index[-1]  \n",
    "    last_surv = surv.loc[final_time_index].values\n",
    "\n",
    "    lower_bound = np.quantile(last_surv, alpha / 2)\n",
    "    upper_bound = np.quantile(last_surv, 1 - alpha / 2)\n",
    "\n",
    "    coverage = np.mean((durations >= lower_bound) & (durations <= upper_bound))\n",
    "    return coverage\n",
    "\n",
    "def evaluate_deephit(main_model, val, X_tests, durations_test, events_test):\n",
    "    surv = main_model.predict_surv_df(X_tests)\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "\n",
    "    time_grid = np.linspace(durations_test.min(), durations_test.max(), 1000)\n",
    "    c_index = ev.concordance_td()\n",
    "    brier_score = ev.integrated_brier_score(time_grid)\n",
    "\n",
    "    X_val, (durations_val, events_val) = val \n",
    "    mean_survival = surv.mean(axis=0).values.ravel()\n",
    "\n",
    "    iso_reg = IsotonicRegression(out_of_bounds = 'clip')\n",
    "    iso_reg.fit(mean_survival, durations_test)\n",
    "\n",
    "    recalibrated_surv = surv.copy()\n",
    "    for col in recalibrated_surv.columns:\n",
    "        recalibrated_surv[col] = iso_reg.transform(surv[col].values)\n",
    "    recalibrated_ev = EvalSurv(recalibrated_surv, durations_test, events_test, censor_surv='km')\n",
    "    recalibrated_c_index = recalibrated_ev.concordance_td()\n",
    "    recalibrated_brier_score = recalibrated_ev.integrated_brier_score(time_grid)\n",
    "\n",
    "    ace = compute_ace(recalibrated_surv, durations_test, events_test, time_grid)\n",
    "    coverage = conformal_coverage(recalibrated_surv, durations_test, events_test, alpha = 0.05)\n",
    "    return c_index, brier_score, recalibrated_c_index, recalibrated_brier_score, ace, coverage\n",
    "\n",
    "def train_deephit(X_train, y_train, val):\n",
    "    input_features, out_features = X_train.shape[1], labtrans.out_features\n",
    "    num_nodes = [128, 128, 64]\n",
    "    batch_norm, output_bias = True, False\n",
    "    dropout = 0.2\n",
    "\n",
    "    deephit = tt.practical.MLPVanilla(input_features, \n",
    "                                    num_nodes, \n",
    "                                    out_features, \n",
    "                                    batch_norm,\n",
    "                                    dropout, \n",
    "                                    output_bias = output_bias)\n",
    "\n",
    "    optimizer = tt.optim.AdamWR(decoupled_weight_decay = 0.01, \n",
    "                                cycle_eta_multiplier = 0.8,\n",
    "                                cycle_multiplier = 2)\n",
    "\n",
    "    main_model = DeepHitSingle(deephit, optimizer, alpha = 0.2, sigma = 0.1, duration_index = labtrans.cuts)\n",
    "    batch_size = 128\n",
    "    epochs = 100\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    verbose = False\n",
    "\n",
    "    lrfind = main_model.lr_finder(X_train, y_train, batch_size, tolerance = 50)\n",
    "    main_model.optimizer.set_lr(0.0001) \n",
    "    log = main_model.fit(X_train, y_train, \n",
    "                        batch_size, \n",
    "                        epochs, \n",
    "                        callbacks, \n",
    "                        verbose,\n",
    "                        val_data = val, \n",
    "                        val_batch_size = batch_size)\n",
    "    return main_model, log\n",
    "\n",
    "def custom_predict(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "    return main_model.net(data).detach().numpy()\n",
    "\n",
    "def get_shap(X_train, X_tests):\n",
    "    X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "    X_tests_np = X_tests.values if hasattr(X_tests, 'values') else X_tests\n",
    "\n",
    "    explainer = shap.Explainer(custom_predict, X_train_np)\n",
    "    shap_values = explainer(X_tests_np)\n",
    "    return shap_values, X_train_np, X_tests_np\n",
    "\n",
    "def get_calibration_plot(y_test, y_calib, calibrated, prediction, pathway):\n",
    "    fraction_of_positives_calibrated, mean_predicted_value_calibrated = calibration_curve(\n",
    "        y_calib['status'], calibrated, n_bins=20)\n",
    "    fraction_of_positives_uncalibrated, mean_predicted_value_uncalibrated = calibration_curve(\n",
    "        y_test['status'], prediction, n_bins=20)\n",
    "\n",
    "    try:\n",
    "        inner, outer = pathway.split('_')[0], pathway.split('_')[2]\n",
    "    except IndexError:\n",
    "        inner, outer = pathway, \"Unknown\"\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(8, 7))\n",
    "    plt.plot(mean_predicted_value_calibrated, fraction_of_positives_calibrated, \n",
    "             marker = 'o', markersize=8, color='blue', label='Calibrated')\n",
    "    plt.plot(mean_predicted_value_uncalibrated, fraction_of_positives_uncalibrated, \n",
    "             marker = 'o', markersize=8, color='red', label='Uncalibrated')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "    \n",
    "    plt.xlabel('Mean Predicted Probability', fontsize=15)\n",
    "    plt.ylabel('Fraction of Positives', fontsize=15)\n",
    "    plt.title(f'{inner} to {outer} Calibration Plot', fontsize=18)\n",
    "    plt.legend(fontsize = 12)\n",
    "    plt.grid(alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_plot(pathway, log, loss_path):\n",
    "    formatted_transition = pathway.replace('_to_', r' $\\rightarrow$ ')\n",
    "    loss_df = log.to_pandas()\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(loss_df.index, loss_df['train_loss'], label = 'Training',        marker = '.', linewidth = 2, color = 'blue')\n",
    "    plt.plot(loss_df.index, loss_df['val_loss'],   label = 'Validation Loss', marker = '.', linewidth = 2, color = 'red')\n",
    "    plt.xlabel('Epochs', fontsize = 15, fontname = 'Arial')\n",
    "    plt.ylabel('Loss',   fontsize = 15, fontname = 'Arial')\n",
    "    plt.title(f'DeepHit Loss: {formatted_transition}', fontsize=18, fontname='Arial')\n",
    "    plt.xticks(fontsize = 13, fontname = 'Arial')\n",
    "    plt.yticks(fontsize = 13, fontname = 'Arial')\n",
    "    plt.legend(fontsize = 13)\n",
    "    plt.savefig(loss_path + f'{pathway}.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()  \n",
    "    \n",
    "def get_shap_local_plot(pathway, shap_values, X_tests_np, df_tests, shap_path):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    rename = pd.read_csv(docs_path + 'rename_columns_forest.csv')\n",
    "    rename_dict = dict(zip(rename['variable'], rename['covariate']))\n",
    "    renaming = df_tests.copy()\n",
    "    renaming = renaming.rename(columns=rename_dict)\n",
    "\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    shap.summary_plot(shap_values, X_tests_np, feature_names = renaming.columns, show = False)\n",
    "    plt.savefig(shap_path + f'local_{pathway}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_shap_globe_plot(pathway, shap_values, X_tests_np, df_tests, shap_path):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    rename = pd.read_csv(docs_path + 'rename_columns_forest.csv')\n",
    "    rename_dict = dict(zip(rename['variable'], rename['covariate']))\n",
    "    renaming = df_tests.copy()\n",
    "    renaming = renaming.rename(columns = rename_dict)\n",
    "\n",
    "    formatted_transition = pathway.replace('_to_', r' $\\rightarrow$ ')\n",
    "    features_df = pd.DataFrame(X_tests_np, columns = renaming.columns[:-2])\n",
    "    feature_means = features_df.mean().sort_values()\n",
    "    colors = feature_means.apply(lambda x: 'blue' if x > 0 else 'red')\n",
    "    plt.figure(figsize = (10, 12))\n",
    "    feature_means.plot(kind = 'barh', color = colors)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.xlabel('Importance Mean', fontsize = 20)\n",
    "    plt.ylabel('Feature', fontsize = 20)\n",
    "    plt.title(f'DeepHit Feature Importance: {formatted_transition}', fontsize = 20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_path + f'global_{pathway}.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_main = resu_path + 'modeling/deephit/'\n",
    "loss_path = save_main + date + '/loss/'\n",
    "shap_path = save_main + date + '/shap/'\n",
    "if not os.path.exists(save_main + date):\n",
    "    os.mkdir(save_main + date)\n",
    "    os.mkdir(loss_path)\n",
    "    os.mkdir(shap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transition</th>\n",
       "      <th>time</th>\n",
       "      <th>cindex</th>\n",
       "      <th>brier</th>\n",
       "      <th>r_cindex</th>\n",
       "      <th>r_brier</th>\n",
       "      <th>ace</th>\n",
       "      <th>coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CKD3A_to_CKD3B</td>\n",
       "      <td>16.092</td>\n",
       "      <td>0.52621</td>\n",
       "      <td>0.251224</td>\n",
       "      <td>0.502848</td>\n",
       "      <td>1928.832388</td>\n",
       "      <td>39.997302</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       transition    time   cindex     brier  r_cindex      r_brier  \\\n",
       "0  CKD3A_to_CKD3B  16.092  0.52621  0.251224  0.502848  1928.832388   \n",
       "\n",
       "         ace coverage  \n",
       "0  39.997302      0.0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CINDEX, BRIER = [], []\n",
    "RINDEX, RRIER = [], []\n",
    "ACE, COVERAGE = [], []\n",
    "TIME = []\n",
    "pathways = long_df['pathway'].unique().tolist()\n",
    "for path in ['CKD3A_to_DEAD', 'CKD3A_to_CKD4', 'CKD3A_to_CKD5A', 'CKD3A_to_CKD5B', 'CKD3B_to_CKD5A', 'CKD3B_to_CKD5B', 'CKD4_to_CKD5B']:\n",
    "    pathways.remove(path)\n",
    "\n",
    "for pathway in pathways:\n",
    "    X_train, y_train, val, X_tests, durations_test, events_test, df_tests = load_input(pathway)\n",
    "\n",
    "    start = time()\n",
    "    main_model, log = train_deephit(X_train, y_train, val)\n",
    "    c_index, brier_score, r_index, recab_brier, ace, coverage = evaluate_deephit(main_model, val, X_tests, durations_test, events_test)\n",
    "    fit_predict_time = np.round(time() - start, 3)\n",
    "\n",
    "    CINDEX.append(c_index), BRIER.append(brier_score)\n",
    "    RINDEX.append(r_index), RRIER.append(recab_brier)\n",
    "\n",
    "    ACE.append(ace), COVERAGE.append(coverage), TIME.append(fit_predict_time)\n",
    "    \n",
    "    shap_values, X_train_np, X_tests_np = get_shap(X_train, X_tests)\n",
    "    get_loss_plot(pathway, log, loss_path)\n",
    "    get_shap_local_plot(pathway, shap_values, X_tests_np, df_tests, shap_path)\n",
    "    get_shap_globe_plot(pathway, shap_values, X_tests_np, df_tests, shap_path)\n",
    "\n",
    "scores = (pathways, TIME, CINDEX, BRIER, RINDEX, RRIER, ACE, COVERAGE)\n",
    "scores = pd.DataFrame(scores).T\n",
    "scores.columns = ['transition', 'time', 'cindex', 'brier', 'r_cindex', 'r_brier', 'ace', 'coverage']\n",
    "scores.to_csv(save_main + date + 'results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
