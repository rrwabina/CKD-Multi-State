{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lifelines import CoxPHFitter, WeibullFitter, WeibullAFTFitter\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import mean_absolute_error, roc_auc_score, precision_score, recall_score, accuracy_score, mean_absolute_percentage_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import CoxPH\n",
    "from pycox.models.loss import CoxPHLoss\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time\n",
    "from sksurv.functions import StepFunction\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score)\n",
    "from sksurv.metrics import brier_score\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.preprocessing import OneHotEncoder, encode_categorical\n",
    "from sksurv.util import Surv\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import KaplanMeierFitter\n",
    "from data import load_dataset \n",
    "from sklearn.metrics import make_scorer\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import shap\n",
    "import time\n",
    "\n",
    "drive = 'G'\n",
    "main_path = drive + ':/Shared drives/CKD_Progression/data/CKD_COHORT_Jan2010_Mar2024_v3.csv'\n",
    "data_path = drive + ':/Shared drives/CKD_Progression/data/'\n",
    "docs_path = drive + ':/Shared drives/CKD_Progression/docs/'\n",
    "save_path = drive + ':/Shared drives/CKD_Progression/save/'\n",
    "resu_path = drive + ':/Shared drives/CKD_Progression/result/'\n",
    "covariates_path = docs_path + 'covariates.csv'\n",
    "removecols_path = docs_path + 'remove_columns.csv'\n",
    "\n",
    "covariates, order_covariates, long_df = load_dataset(get_columns = True)\n",
    "\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from pycox.models import CoxPH\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from pycox.evaluation import EvalSurv\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "_ = torch.manual_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                self.early_stop = True\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input(pathway):\n",
    "    transition_df = long_df[long_df['pathway'] == pathway]\n",
    "    univariate  = pd.read_excel(resu_path + f'univariate/LR_test/{pathway}.xlsx', sheet_name = pathway)\n",
    "    multivariate_covariates = univariate[univariate['pvalue'] < 0.20]['variable'].tolist()\n",
    "    transition_df = transition_df[multivariate_covariates + ['time', 'status']]\n",
    "    df_train = transition_df.copy()\n",
    "    df_tests = df_train.sample(frac = 0.2, random_state=42)\n",
    "    df_train = df_train.drop(df_tests.index)\n",
    "    df_valid = df_train.sample(frac = 0.2, random_state=42)\n",
    "    df_train = df_train.drop(df_valid.index)\n",
    "\n",
    "    standardize = [([col], StandardScaler()) for col in multivariate_covariates]\n",
    "    x_mapper = DataFrameMapper(standardize)\n",
    "\n",
    "    X_train = x_mapper.fit_transform(df_train).astype('float32')\n",
    "    X_valid = x_mapper.transform(df_valid).astype('float32')\n",
    "    X_tests = x_mapper.transform(df_tests).astype('float32')\n",
    "\n",
    "    get_target = lambda df: (df['time'].values, df['status'].values)\n",
    "    y_train = get_target(df_train)\n",
    "    y_valid = get_target(df_valid)\n",
    "    durations_test, events_test = get_target(df_tests)\n",
    "    val = X_valid, y_valid\n",
    "    return X_train, y_train, val, X_tests, durations_test, events_test, df_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_plot(pathway, log, loss_path):\n",
    "    formatted_transition = pathway.replace('_to_', r' $\\rightarrow$ ')\n",
    "    loss_df = log.to_pandas()\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.plot(loss_df.index, loss_df['train_loss'], label = 'Training',        marker = '.', linewidth = 2, color = 'blue')\n",
    "    plt.plot(loss_df.index, loss_df['val_loss'],   label = 'Validation Loss', marker = '.', linewidth = 2, color = 'red')\n",
    "    plt.xlabel('Epochs', fontsize = 15, fontname = 'Arial')\n",
    "    plt.ylabel('Loss',   fontsize = 15, fontname = 'Arial')\n",
    "    plt.title(f'DeepSurv Loss: {formatted_transition}', fontsize=18, fontname='Arial')\n",
    "    plt.xticks(fontsize = 13, fontname = 'Arial')\n",
    "    plt.yticks(fontsize = 13, fontname = 'Arial')\n",
    "    plt.legend(fontsize = 13)\n",
    "    plt.savefig(loss_path + f'{pathway}.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()  \n",
    "\n",
    "def train_deepsurv(X_train, y_train, val):\n",
    "    input_features, out_features = X_train.shape[1], 1\n",
    "    num_nodes = [64, 64]\n",
    "    batch_norm, output_bias = True, False\n",
    "    dropout = 0.2\n",
    "\n",
    "    deepsurv = tt.practical.MLPVanilla(input_features, \n",
    "                                    num_nodes, \n",
    "                                    out_features, \n",
    "                                    batch_norm,\n",
    "                                    dropout, \n",
    "                                    output_bias = output_bias)\n",
    "\n",
    "    optimizer = tt.optim.AdamWR(decoupled_weight_decay = 0.01, \n",
    "                                cycle_eta_multiplier = 0.8,\n",
    "                                cycle_multiplier = 2)\n",
    "\n",
    "    main_model = CoxPH(deepsurv, optimizer)\n",
    "    batch_size = 128\n",
    "    epochs = 100\n",
    "    callbacks = [tt.callbacks.EarlyStopping()]\n",
    "    verbose = False\n",
    "\n",
    "    lrfind = main_model.lr_finder(X_train, y_train, batch_size, tolerance = 50)\n",
    "    main_model.optimizer.set_lr(0.0001) \n",
    "    log = main_model.fit(X_train, y_train, \n",
    "                        batch_size, \n",
    "                        epochs, \n",
    "                        callbacks, \n",
    "                        verbose,\n",
    "                        val_data = val, \n",
    "                        val_batch_size = batch_size)\n",
    "    return main_model, log\n",
    "    \n",
    "def evaluate_deepsurv(main_model, val, X_tests, durations_test, events_test):\n",
    "    main_model.partial_log_likelihood(*val).mean()\n",
    "    _ = main_model.compute_baseline_hazards()\n",
    "    surv = main_model.predict_surv_df(X_tests)\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv = 'km')\n",
    "\n",
    "    time_grid = np.linspace(durations_test.min(), durations_test.max(), 1000)\n",
    "    c_index = ev.concordance_td()\n",
    "    brier_score = ev.integrated_brier_score(time_grid)\n",
    "    return c_index, brier_score\n",
    "\n",
    "def compute_ace(surv, durations, events, time_grid):\n",
    "    ace = 0\n",
    "    n_bins = len(time_grid)\n",
    "    for t in time_grid:\n",
    "        nearest_time_index = surv.index.get_loc(t, method='nearest')\n",
    "        pred_surv = surv.iloc[nearest_time_index].values\n",
    "    \n",
    "        observed = (durations >= t).astype(int)\n",
    "        observed_censored = observed[events == 1]\n",
    "        pred_surv_censored = pred_surv[events == 1]\n",
    "        bin_error = np.abs(pred_surv_censored.mean() - observed_censored.mean())\n",
    "        ace += bin_error\n",
    "    \n",
    "    return ace / n_bins\n",
    "\n",
    "def conformal_coverage(surv, durations, events, alpha):\n",
    "    final_time_index = surv.index[-1]  \n",
    "    last_surv = surv.loc[final_time_index].values\n",
    "\n",
    "    lower_bound = np.quantile(last_surv, alpha / 2)\n",
    "    upper_bound = np.quantile(last_surv, 1 - alpha / 2)\n",
    "\n",
    "    coverage = np.mean((durations >= lower_bound) & (durations <= upper_bound))\n",
    "    return coverage\n",
    "\n",
    "def evaluate_deepsurv(main_model, val, X_tests, durations_test, events_test):\n",
    "    main_model.partial_log_likelihood(*val).mean()\n",
    "    _ = main_model.compute_baseline_hazards()\n",
    "    \n",
    "    surv = main_model.predict_surv_df(X_tests)\n",
    "    ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
    "\n",
    "    time_grid = np.linspace(durations_test.min(), durations_test.max(), 1000)\n",
    "    c_index = ev.concordance_td()\n",
    "    brier_score = ev.integrated_brier_score(time_grid)\n",
    "\n",
    "    X_val, (durations_val, events_val) = val \n",
    "    mean_survival = surv.mean(axis=0).values.ravel()\n",
    "\n",
    "    iso_reg = IsotonicRegression(out_of_bounds = 'clip')\n",
    "    iso_reg.fit(mean_survival, durations_test)\n",
    "\n",
    "    recalibrated_surv = surv.copy()\n",
    "    for col in recalibrated_surv.columns:\n",
    "        recalibrated_surv[col] = iso_reg.transform(surv[col].values)\n",
    "    recalibrated_ev = EvalSurv(recalibrated_surv, durations_test, events_test, censor_surv='km')\n",
    "    recalibrated_c_index = recalibrated_ev.concordance_td()\n",
    "    recalibrated_brier_score = recalibrated_ev.integrated_brier_score(time_grid)\n",
    "\n",
    "    ace = compute_ace(recalibrated_surv, durations_test, events_test, time_grid)\n",
    "    coverage = conformal_coverage(recalibrated_surv, durations_test, events_test, alpha = 0.05)\n",
    "\n",
    "    return c_index, brier_score, recalibrated_c_index, recalibrated_brier_score, ace, coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_predict(data):\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "    return main_model.net(data).detach().numpy()\n",
    "\n",
    "def get_shap(X_train, X_tests):\n",
    "    X_train_np = X_train.values if hasattr(X_train, 'values') else X_train\n",
    "    X_tests_np = X_tests.values if hasattr(X_tests, 'values') else X_tests\n",
    "\n",
    "    explainer = shap.Explainer(custom_predict, X_train_np)\n",
    "    shap_values = explainer(X_tests_np)\n",
    "    return shap_values, X_train_np, X_tests_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap_local_plot(pathway, shap_values, X_tests_np, df_tests, shap_path):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    rename = pd.read_csv(docs_path + 'rename_columns_forest.csv')\n",
    "    rename_dict = dict(zip(rename['variable'], rename['covariate']))\n",
    "    renaming = df_tests.copy()\n",
    "    renaming = renaming.rename(columns=rename_dict)\n",
    "\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    shap.summary_plot(shap_values, X_tests_np, feature_names = renaming.columns, show = False)\n",
    "    plt.savefig(shap_path + f'local_{pathway}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def get_shap_globe_plot(pathway, shap_values, X_tests_np, df_tests, shap_path):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    rename = pd.read_csv(docs_path + 'rename_columns_forest.csv')\n",
    "    rename_dict = dict(zip(rename['variable'], rename['covariate']))\n",
    "    renaming = df_tests.copy()\n",
    "    renaming = renaming.rename(columns = rename_dict)\n",
    "\n",
    "    formatted_transition = pathway.replace('_to_', r' $\\rightarrow$ ')\n",
    "    features_df = pd.DataFrame(X_tests_np, columns = renaming.columns[:-2])\n",
    "    feature_means = features_df.mean().sort_values()\n",
    "    feature_means.to_csv(shap_path + f'importance_{pathway}.csv')\n",
    "    colors = feature_means.apply(lambda x: 'blue' if x > 0 else 'red')\n",
    "    plt.figure(figsize = (10, 12))\n",
    "    feature_means.plot(kind = 'barh', color = colors)\n",
    "    plt.yticks(fontsize = 16)\n",
    "    plt.xticks(fontsize = 14)\n",
    "    plt.xlabel('Importance Mean', fontsize = 20)\n",
    "    plt.ylabel('Feature', fontsize = 20)\n",
    "    plt.title(f'DeepSurv Feature Importance: {formatted_transition}', fontsize = 20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(shap_path + f'global_{pathway}.png', dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "save_main = resu_path + 'modeling/deepsurv/'\n",
    "loss_path = save_main + date + '/loss/'\n",
    "shap_path = save_main + date + '/shap/'\n",
    "if not os.path.exists(save_main + date):\n",
    "    os.mkdir(save_main + date)\n",
    "    os.mkdir(loss_path)\n",
    "    os.mkdir(shap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Permutation explainer: 4687it [03:28, 21.25it/s]                          \n",
      "Permutation explainer: 4687it [03:19, 22.19it/s]                          \n",
      "Permutation explainer: 1255it [01:10, 15.08it/s]                          \n",
      "Permutation explainer: 2180it [02:03, 16.09it/s]                          \n",
      "Permutation explainer: 2422it [02:07, 17.41it/s]                          \n",
      "Exact explainer: 2484it [06:31,  6.17it/s]                          \n",
      "Permutation explainer: 2723it [03:35, 11.86it/s]                          \n",
      "Permutation explainer: 2934it [02:23, 19.37it/s]                          \n",
      "Permutation explainer: 2723it [02:56, 14.35it/s]                          \n",
      "Permutation explainer: 2934it [04:47,  9.84it/s]                          \n",
      "Permutation explainer: 1387it [01:40, 12.29it/s]                          \n",
      "Permutation explainer: 1235it [01:50, 10.13it/s]                          \n",
      "Permutation explainer: 1387it [01:44, 11.95it/s]                          \n",
      "Permutation explainer: 844it [00:55, 12.70it/s]                         \n",
      "Permutation explainer: 781it [01:04, 10.16it/s]                         \n",
      "Exact explainer: 844it [01:07, 10.53it/s]                         \n",
      "Permutation explainer: 632it [00:42, 11.26it/s]                         \n",
      "Permutation explainer: 658it [00:54,  9.84it/s]                         \n"
     ]
    }
   ],
   "source": [
    "CINDEX, BRIER = [], []\n",
    "RINDEX, RRIER = [], []\n",
    "ACE, COVERAGE = [], []\n",
    "TIME = []\n",
    "pathways = long_df['pathway'].unique().tolist()\n",
    "for path in ['CKD3A_to_DEAD', 'CKD3A_to_CKD4', 'CKD3A_to_CKD5A', 'CKD3A_to_CKD5B', 'CKD3B_to_CKD5A', 'CKD3B_to_CKD5B', 'CKD4_to_CKD5B']:\n",
    "    pathways.remove(path)\n",
    "\n",
    "for pathway in pathways:\n",
    "    X_train, y_train, val, X_tests, durations_test, events_test, df_tests = load_input(pathway)\n",
    "\n",
    "    start = time.time()\n",
    "    main_model, log = train_deepsurv(X_train, y_train, val)\n",
    "    c_index, brier_score, r_index, recab_brier, ace, coverage = evaluate_deepsurv(main_model, val, X_tests, durations_test, events_test)\n",
    "    fit_predict_time = np.round(time.time() - start, 3)\n",
    "\n",
    "    CINDEX.append(c_index), BRIER.append(brier_score)\n",
    "    RINDEX.append(r_index), RRIER.append(recab_brier)\n",
    "\n",
    "    ACE.append(ace), COVERAGE.append(coverage), TIME.append(fit_predict_time)\n",
    "    \n",
    "    shap_values, X_train_np, X_tests_np = get_shap(X_train, X_tests)\n",
    "    get_loss_plot(pathway, log, loss_path)\n",
    "    get_shap_local_plot(pathway, shap_values, X_tests_np, df_tests, shap_path)\n",
    "    get_shap_globe_plot(pathway, shap_values, X_tests_np, df_tests, shap_path)\n",
    "\n",
    "scores = (pathways, TIME, CINDEX, BRIER, RINDEX, RRIER, ACE, COVERAGE)\n",
    "scores = pd.DataFrame(scores).T\n",
    "scores.columns = ['transition', 'time', 'cindex', 'brier', 'r_cindex', 'r_brier', 'ace', 'coverage']\n",
    "scores.to_csv(save_main + date + 'results.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
